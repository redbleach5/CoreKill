"""–°–∏—Å—Ç–µ–º–∞ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∞–ø–æ–≤ workflow
–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –æ—Ü–µ–Ω–∫–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∂–µ–ª–µ–∑–æ.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ë–µ–Ω—á–º–∞—Ä–∫ LLM –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
- –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É —ç—Ç–∞–ø—É
- –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –£—á—ë—Ç –º–æ–¥–µ–ª–∏ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏
"""
import time
import json
import asyncio
from pathlib import Path
from dataclasses import dataclass, field, asdict
from typing import Dict, Optional, List
from datetime import datetime, timezone
from statistics import mean, median, stdev

from utils.logger import get_logger
from utils.config import get_config

logger = get_logger()


@dataclass
class StageMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ workflow."""
    stage_name: str
    samples: List[float] = field(default_factory=list)  # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    # –õ–∏–º–∏—Ç —Å—ç–º–ø–ª–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ)
    MAX_SAMPLES = 100
    
    def add_sample(self, duration: float) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏."""
        self.samples.append(duration)
        # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ MAX_SAMPLES
        if len(self.samples) > self.MAX_SAMPLES:
            self.samples = self.samples[-self.MAX_SAMPLES:]
    
    @property
    def count(self) -> int:
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–º–µ—Ä–æ–≤."""
        return len(self.samples)
    
    @property
    def avg(self) -> float:
        """–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è."""
        return mean(self.samples) if self.samples else 0.0
    
    @property
    def median_time(self) -> float:
        """–ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è."""
        return median(self.samples) if self.samples else 0.0
    
    @property
    def std_dev(self) -> float:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ."""
        return stdev(self.samples) if len(self.samples) > 1 else 0.0
    
    @property
    def min_time(self) -> float:
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è."""
        return min(self.samples) if self.samples else 0.0
    
    @property
    def max_time(self) -> float:
        """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è."""
        return max(self.samples) if self.samples else 0.0
    
    def to_dict(self) -> Dict:
        """–°–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä—å."""
        return {
            "stage_name": self.stage_name,
            "count": self.count,
            "avg": round(self.avg, 2),
            "median": round(self.median_time, 2),
            "std_dev": round(self.std_dev, 2),
            "min": round(self.min_time, 2),
            "max": round(self.max_time, 2),
            "samples": self.samples[-20:]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'StageMetrics':
        """–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è."""
        metrics = cls(stage_name=data["stage_name"])
        metrics.samples = data.get("samples", [])
        return metrics


@dataclass
class SystemBenchmark:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã."""
    # –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 100 —Ç–æ–∫–µ–Ω–æ–≤ (–±–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç)
    tokens_per_second: float = 0.0
    # –í—Ä–µ–º—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (latency)
    time_to_first_token: float = 0.0
    # –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞
    model_used: str = ""
    # –ö–æ–≥–¥–∞ –ø—Ä–æ–≤–æ–¥–∏–ª—Å—è –±–µ–Ω—á–º–∞—Ä–∫
    timestamp: str = ""
    # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–≥–æ (1.0 = —Å—Ä–µ–¥–Ω–µ–µ –∂–µ–ª–µ–∑–æ)
    performance_multiplier: float = 1.0
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'SystemBenchmark':
        return cls(**data)


class PerformanceMetrics:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    
    –§—É–Ω–∫—Ü–∏–∏:
    - –ë–µ–Ω—á–º–∞—Ä–∫ LLM –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
    - –°–±–æ—Ä —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —ç—Ç–∞–ø–∞–º
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
    - –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
    """
    
    # –ë–∞–∑–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ (–¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∂–µ–ª–µ–∑–∞, tokens/sec ~20)
    BASE_STAGE_DURATIONS = {
        "intent": 3.0,
        "planning": 8.0,
        "research": 12.0,
        "testing": 15.0,
        "coding": 25.0,
        "validation": 5.0,
        "debug": 10.0,
        "fixing": 15.0,
        "reflection": 5.0,
        "critic": 5.0,
        "chat": 5.0,
        "greeting": 1.0,
        "help": 2.0
    }
    
    # –ë–∞–∑–æ–≤–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å (—Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É) –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
    BASE_TOKENS_PER_SECOND = 20.0
    
    def __init__(self, persist_path: Optional[str] = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–Ω–µ–¥–∂–µ—Ä –º–µ—Ç—Ä–∏–∫.
        
        Args:
            persist_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ (None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        """
        config = get_config()
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ persist_path –∏ config.output_dir –Ω–µ —è–≤–ª—è—é—Ç—Å—è Mock –æ–±—ä–µ–∫—Ç–∞–º–∏
        from utils.test_mode import is_test_mode
        
        if persist_path and isinstance(persist_path, str):
            self.persist_path = Path(persist_path)
        else:
            output_dir = getattr(config, 'output_dir', None)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ output_dir –Ω–µ —è–≤–ª—è–µ—Ç—Å—è Mock –æ–±—ä–µ–∫—Ç–æ–º
            if output_dir and isinstance(output_dir, str):
                self.persist_path = Path(output_dir) / "metrics"
            else:
                # Fallback –Ω–∞ —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ config.output_dir - Mock –∏–ª–∏ –≤ —Ç–µ—Å—Ç–∞—Ö
                if is_test_mode():
                    # –í —Ç–µ—Å—Ç–∞—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                    import tempfile
                    self.persist_path = Path(tempfile.gettempdir()) / "test_metrics"
                else:
                    self.persist_path = Path.cwd() / "output" / "metrics"
        
        # –í —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –Ω–µ —Å–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–æ–π —Å –ø—Ä–∞–≤–∞–º–∏)
        if not is_test_mode():
            self.persist_path.mkdir(parents=True, exist_ok=True)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç—Ç–∞–ø–∞–º
        self.stage_metrics: Dict[str, StageMetrics] = {}
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
        self.benchmark: Optional[SystemBenchmark] = None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self._load()
        
        logger.info(f"‚úÖ PerformanceMetrics –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–ø—É—Ç—å: {self.persist_path})")
    
    def _load(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å –¥–∏—Å–∫–∞."""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
        benchmark_file = self.persist_path / "benchmark.json"
        if benchmark_file.exists():
            try:
                with open(benchmark_file, "r") as f:
                    data = json.load(f)
                    self.benchmark = SystemBenchmark.from_dict(data)
                    logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫: {self.benchmark.tokens_per_second:.1f} tok/s")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —ç—Ç–∞–ø–æ–≤
        metrics_file = self.persist_path / "stage_metrics.json"
        if metrics_file.exists():
            try:
                with open(metrics_file, "r") as f:
                    data = json.load(f)
                    for stage_data in data.get("stages", []):
                        metrics = StageMetrics.from_dict(stage_data)
                        self.stage_metrics[metrics.stage_name] = metrics
                    logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.stage_metrics)} –º–µ—Ç—Ä–∏–∫ —ç—Ç–∞–ø–æ–≤")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç—Ä–∏–∫: {e}")
    
    def _save(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –¥–∏—Å–∫."""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–Ω—á–º–∞—Ä–∫
        if self.benchmark:
            benchmark_file = self.persist_path / "benchmark.json"
            try:
                with open(benchmark_file, "w") as f:
                    json.dump(self.benchmark.to_dict(), f, indent=2)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —ç—Ç–∞–ø–æ–≤
        metrics_file = self.persist_path / "stage_metrics.json"
        try:
            data = {
                "stages": [m.to_dict() for m in self.stage_metrics.values()],
                "updated_at": datetime.now(timezone.utc).isoformat()
            }
            with open(metrics_file, "w") as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {e}")
    
    async def run_benchmark(self, model: Optional[str] = None) -> SystemBenchmark:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ LLM –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏.
        
        Args:
            model: –ú–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (None = —Ç–µ–∫—É—â–∞—è)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
        """
        from infrastructure.local_llm import LocalLLM
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if not model:
            config = get_config()
            model = config.default_model
        
        logger.info(f"üîß –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ LLM (–º–æ–¥–µ–ª—å: {model})...")
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∞
        test_prompt = "–ù–∞–ø–∏—à–∏ —á–∏—Å–ª–∞ –æ—Ç 1 –¥–æ 20, –∫–∞–∂–¥–æ–µ –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ."
        
        try:
            llm = LocalLLM(model=model, temperature=0.1)
            
            # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
            start_time = time.time()
            response = await asyncio.to_thread(
                llm.generate,
                test_prompt,
                num_predict=100  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ~100 —Ç–æ–∫–µ–Ω–æ–≤
            )
            total_time = time.time() - start_time
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞)
            estimated_tokens = len(response) / 4
            tokens_per_second = estimated_tokens / total_time if total_time > 0 else 0
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance_multiplier = tokens_per_second / self.BASE_TOKENS_PER_SECOND
            
            self.benchmark = SystemBenchmark(
                tokens_per_second=round(tokens_per_second, 2),
                time_to_first_token=round(total_time / 10, 3),  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
                model_used=model,
                timestamp=datetime.now(timezone.utc).isoformat(),
                performance_multiplier=round(performance_multiplier, 2)
            )
            
            self._save()
            
            logger.info(
                f"‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à—ë–Ω: {tokens_per_second:.1f} tok/s, "
                f"–º–Ω–æ–∂–∏—Ç–µ–ª—å: {performance_multiplier:.2f}x"
            )
            
            return self.benchmark
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}", error=e)
            # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            self.benchmark = SystemBenchmark(
                tokens_per_second=self.BASE_TOKENS_PER_SECOND,
                model_used=model,
                timestamp=datetime.now(timezone.utc).isoformat(),
                performance_multiplier=1.0
            )
            return self.benchmark
    
    def record_stage_duration(self, stage: str, duration: float) -> None:
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∞–ø–∞.
        
        Args:
            stage: –ù–∞–∑–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–∞
            duration: –í—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        if stage not in self.stage_metrics:
            self.stage_metrics[stage] = StageMetrics(stage_name=stage)
        
        self.stage_metrics[stage].add_sample(duration)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 10 –∑–∞–º–µ—Ä–æ–≤
        if self.stage_metrics[stage].count % 10 == 0:
            self._save()
    
    def get_estimated_duration(self, stage: str) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —ç—Ç–∞–ø–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –±–∞–∑–æ–≤—ã–µ * –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç.
        
        Args:
            stage: –ù–∞–∑–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–∞
            
        Returns:
            –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (>5 –∑–∞–º–µ—Ä–æ–≤) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–¥–∏–∞–Ω—É
        if stage in self.stage_metrics and self.stage_metrics[stage].count >= 5:
            return self.stage_metrics[stage].median_time
        
        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º
        base_duration = self.BASE_STAGE_DURATIONS.get(stage, 5.0)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–±—ã—Å—Ç—Ä–µ–µ –∂–µ–ª–µ–∑–æ = –º–µ–Ω—å—à–µ –≤—Ä–µ–º—è)
        if self.benchmark and self.benchmark.performance_multiplier > 0:
            return base_duration / self.benchmark.performance_multiplier
        
        return base_duration
    
    def get_all_estimates(self) -> Dict[str, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {stage: estimated_seconds}
        """
        return {
            stage: self.get_estimated_duration(stage)
            for stage in self.BASE_STAGE_DURATIONS.keys()
        }
    
    def get_metrics_summary(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –º–µ—Ç—Ä–∏–∫ –¥–ª—è API.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        return {
            "benchmark": self.benchmark.to_dict() if self.benchmark else None,
            "stages": {
                name: metrics.to_dict()
                for name, metrics in self.stage_metrics.items()
            },
            "estimates": self.get_all_estimates(),
            "has_calibration": self.benchmark is not None,
            "total_samples": sum(m.count for m in self.stage_metrics.values())
        }


# Singleton
_performance_metrics: Optional[PerformanceMetrics] = None


def get_performance_metrics() -> PerformanceMetrics:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç singleton PerformanceMetrics.
    
    Returns:
        –≠–∫–∑–µ–º–ø–ª—è—Ä PerformanceMetrics
    """
    global _performance_metrics
    if _performance_metrics is None:
        _performance_metrics = PerformanceMetrics()
    return _performance_metrics


def reset_performance_metrics() -> None:
    """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç singleton PerformanceMetrics."""
    global _performance_metrics
    _performance_metrics = None
