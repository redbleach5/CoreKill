"""–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ LLM —á–µ—Ä–µ–∑ Ollama.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã:
- –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π (generate) ‚Äî –¥–ª—è CLI –∏ –ø—Ä–æ—Å—Ç—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π (generate_async) ‚Äî –¥–ª—è FastAPI –∏ –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —É–¥–∞–ª—ë–Ω–Ω–æ–≥–æ Ollama:
- –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ config.toml [ollama] —Å–µ–∫—Ü–∏—é
- –î–ª—è —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å–µ—Ç—è–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Tailscale (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –†–§)

Structured Output:
- generate_structured() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Pydantic –º–æ–¥–µ–ª–∏ —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç format="json" Ollama –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ JSON

Reasoning Models:
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ DeepSeek-R1, QwQ —Å <think> –±–ª–æ–∫–∞–º–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ reasoning —á–µ—Ä–µ–∑ reasoning_utils

–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º,
–∞ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å httpx —á–µ—Ä–µ–∑ OllamaConnectionPool –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""
import asyncio
import json
import os
import ollama
from typing import Optional, Dict, Any, Type, TypeVar
import time
import threading
import concurrent.futures

from pydantic import BaseModel, ValidationError

from utils.logger import get_logger


logger = get_logger()


def _configure_ollama_host() -> None:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ö–æ—Å—Ç Ollama –∏–∑ config.toml.
    
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OLLAMA_HOST –∫–æ—Ç–æ—Ä—É—é
    –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ollama Python SDK.
    """
    # –ù–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –µ—Å–ª–∏ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤—Ä—É—á–Ω—É—é
    if os.environ.get("OLLAMA_HOST"):
        return
    
    try:
        from utils.config import get_config
        config = get_config()
        host = config.ollama_host
        
        if host and host != "http://localhost:11434":
            os.environ["OLLAMA_HOST"] = host
            logger.info(f"üåê Ollama —Ö–æ—Å—Ç: {host}")
    except Exception:
        pass  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç


# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
_configure_ollama_host()


class LLMTimeoutError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è —Ç–∞–π–º–∞—É—Ç–∞ LLM –∑–∞–ø—Ä–æ—Å–∞."""
    pass


class StructuredOutputError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ structured output."""
    pass


from dataclasses import dataclass


@dataclass
class StreamChunk:
    """–ß–∞–Ω–∫ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –æ—Ç LLM.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è real-time —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ UI.
    –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–¥–µ–ª—è—Ç—å <think> –±–ª–æ–∫–∏ –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
    
    Attributes:
        content: –¢–µ–∫—Å—Ç —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
        is_thinking: True –µ—Å–ª–∏ —á–∞–Ω–∫ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ <think> –±–ª–æ–∫–∞
        is_done: True –µ—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        full_response: –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
    """
    content: str
    is_thinking: bool
    is_done: bool
    full_response: str


# TypeVar –¥–ª—è generic generate_structured
T = TypeVar('T', bound=BaseModel)


class LocalLLM:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama API.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç retry —Å exponential backoff, –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫.
    """

    # –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è exponential backoff (—Å–µ–∫—É–Ω–¥—ã)
    BASE_RETRY_DELAY = 1.0
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É retry
    MAX_RETRY_DELAY = 30.0
    
    # –û–±—â–∏–π ThreadPoolExecutor –¥–ª—è –≤—Å–µ—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∫–ª–∞—Å—Å–∞
    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö ollama –≤—ã–∑–æ–≤–æ–≤ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
    _executor: Optional[concurrent.futures.ThreadPoolExecutor] = None
    _executor_lock = threading.Lock()
    _executor_max_workers = 10  # –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    def __init__(
        self,
        model: str,
        temperature: float = 0.25,
        top_p: float = 0.9,
        timeout: int = 120,
        max_retries: int = 3
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LocalLLM.
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.15-0.35 –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º)
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 120—Å)
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        """
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        self.timeout = timeout
        self.max_retries = max_retries
    
    @classmethod
    def _get_executor(cls) -> concurrent.futures.ThreadPoolExecutor:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–∏–π ThreadPoolExecutor –¥–ª—è –∫–ª–∞—Å—Å–∞.
        
        –°–æ–∑–¥–∞—ë—Ç executor –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è).
        –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–µ–Ω.
        
        Returns:
            ThreadPoolExecutor –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        if cls._executor is None:
            with cls._executor_lock:
                # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                if cls._executor is None:
                    cls._executor = concurrent.futures.ThreadPoolExecutor(
                        max_workers=cls._executor_max_workers,
                        thread_name_prefix="LocalLLM"
                    )
                    logger.debug(f"‚úÖ ThreadPoolExecutor —Å–æ–∑–¥–∞–Ω ({cls._executor_max_workers} workers)")
        return cls._executor
    
    @classmethod
    def shutdown_executor(cls) -> None:
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –æ–±—â–∏–π ThreadPoolExecutor.
        
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ graceful shutdown –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
        """
        with cls._executor_lock:
            if cls._executor is not None:
                cls._executor.shutdown(wait=True)
                cls._executor = None
                logger.info("‚úÖ ThreadPoolExecutor –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def _calculate_backoff(self, attempt: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è exponential backoff.
        
        Args:
            attempt: –ù–æ–º–µ—Ä –ø–æ–ø—ã—Ç–∫–∏ (–Ω–∞—á–∏–Ω–∞—è —Å 0)
            
        Returns:
            –ó–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        delay = self.BASE_RETRY_DELAY * (2 ** attempt)
        return min(delay, self.MAX_RETRY_DELAY)

    def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: int = 4096,
        format: Optional[str] = None,
        **kwargs: Any
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            format: –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ("json" –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ JSON)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ollama.generate
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        temp = temperature if temperature is not None else self.temperature
        tp = top_p if top_p is not None else self.top_p
        
        options: Dict[str, Any] = {
            "temperature": temp,
            "top_p": tp,
            "num_predict": num_predict
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        options.update(kwargs.get("options", {}))
        
        last_error: Optional[Exception] = None
        
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á (intent, planning) —É–º–µ–Ω—å—à–∞–µ–º num_predict –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if num_predict > 1024 and len(prompt) < 500:
            options["num_predict"] = min(512, num_predict // 2)
        
        for attempt in range(self.max_retries + 1):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω (—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–µ)
                if attempt == 0:
                    try:
                        ollama.list()
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ —Å–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω: {e}")
                        return ""
                
                start_time = time.time()
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è ollama.generate
                generate_kwargs = {
                    "model": self.model,
                    "prompt": prompt,
                    "options": options,
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º format –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω (–¥–ª—è JSON output)
                if format:
                    generate_kwargs["format"] = format
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ kwargs (–∫—Ä–æ–º–µ options –∏ format)
                for k, v in kwargs.items():
                    if k not in ("options", "format"):
                        generate_kwargs[k] = v
                
                # –í—ã–∑–æ–≤ —Å timeout —á–µ—Ä–µ–∑ –æ–±—â–∏–π ThreadPoolExecutor (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ª—é–±–æ–º –ø–æ—Ç–æ–∫–µ)
                executor = self._get_executor()
                future = executor.submit(
                    ollama.generate,  # type: ignore[arg-type]
                    **generate_kwargs
                )
                
                try:
                    response = future.result(timeout=self.timeout)
                except concurrent.futures.TimeoutError:
                    elapsed = time.time() - start_time
                    logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç LLM –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                    raise LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                
                elapsed = time.time() - start_time
                result = response.get("response", "").strip()
                
                if result:
                    logger.debug(f"‚úÖ LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.1f}—Å ({len(result)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                    
            except (LLMTimeoutError, concurrent.futures.TimeoutError):
                last_error = LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å...")
                    time.sleep(backoff)
                    continue
                    
            except Exception as e:
                last_error = e
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {e}")
                    time.sleep(backoff)
                    continue
                else:
                    break
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
        error_msg = f"–û—à–∏–±–∫–∞ Ollama –ø–æ—Å–ª–µ {self.max_retries + 1} –ø–æ–ø—ã—Ç–æ–∫: {last_error}"
        logger.error(f"‚ùå {error_msg}", error=last_error)
        return ""

    def chat(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user", "content": "..."}]
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        temp = temperature if temperature is not None else self.temperature
        tp = top_p if top_p is not None else self.top_p
        
        options: Dict[str, Any] = {
            "temperature": temp,
            "top_p": tp
        }
        options.update(kwargs.get("options", {}))
        
        last_error: Optional[Exception] = None
        
        for attempt in range(self.max_retries + 1):
            try:
                start_time = time.time()
                
                # –í—ã–∑–æ–≤ —Å timeout —á–µ—Ä–µ–∑ –æ–±—â–∏–π ThreadPoolExecutor
                executor = self._get_executor()
                future = executor.submit(
                    ollama.chat,  # type: ignore[arg-type]
                    model=self.model,
                    messages=messages,
                    options=options,
                    **{k: v for k, v in kwargs.items() if k != "options"}
                )
                
                try:
                    response = future.result(timeout=self.timeout)
                except concurrent.futures.TimeoutError:
                    elapsed = time.time() - start_time
                    logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç chat –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                    raise LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                
                result = response.get("message", {}).get("content", "").strip()
                if result:
                    return result
                    
            except (LLMTimeoutError, concurrent.futures.TimeoutError):
                last_error = LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Chat retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å...")
                    time.sleep(backoff)
                    continue
                    
            except Exception as e:
                last_error = e
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Chat retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {e}")
                    time.sleep(backoff)
                    continue
                else:
                    break
        
        error_msg = f"–û—à–∏–±–∫–∞ Ollama chat –ø–æ—Å–ª–µ {self.max_retries + 1} –ø–æ–ø—ã—Ç–æ–∫: {last_error}"
        logger.error(f"‚ùå {error_msg}", error=last_error)
        return ""
    
    # === STRUCTURED OUTPUT ===
    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Pydantic –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–æ–≤
    
    def generate_structured(
        self,
        prompt: str,
        response_model: Type[T],
        num_predict: int = 1024,
        retries: int = 2
    ) -> T:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å Pydantic –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç format="json" Ollama –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ JSON —Ñ–æ—Ä–º–∞—Ç–∞,
        –∑–∞—Ç–µ–º –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç —á–µ—Ä–µ–∑ Pydantic –º–æ–¥–µ–ª—å.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            response_model: Pydantic –º–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
            num_predict: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            –ü—Ä–æ–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Pydantic –æ–±—ä–µ–∫—Ç
            
        Raises:
            StructuredOutputError: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç
            
        Example:
            from models import IntentResponse
            
            response = llm.generate_structured(
                "Classify: –Ω–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é",
                IntentResponse
            )
            print(response.intent)  # "create"
        """
        schema = response_model.model_json_schema()
        schema_str = json.dumps(schema, indent=2)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ —Ñ–æ—Ä–º–∞—Ç—É –≤ –ø—Ä–æ–º–ø—Ç
        enhanced_prompt = f"""{prompt}

IMPORTANT: Return response as valid JSON matching this schema:
{schema_str}

JSON:"""
        
        last_error: Optional[Exception] = None
        
        for attempt in range(retries + 1):
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å format="json"
                response = self.generate(
                    prompt=enhanced_prompt,
                    num_predict=num_predict,
                    format="json"  # Ollama –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON
                )
                
                if not response:
                    raise StructuredOutputError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM")
                
                # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                # –ò–Ω–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON
                json_str = self._extract_json(response)
                
                # Pydantic –≤–∞–ª–∏–¥–∞—Ü–∏—è
                result = response_model.model_validate_json(json_str)
                
                logger.debug(
                    f"‚úÖ Structured output —É—Å–ø–µ—à–Ω–æ: {response_model.__name__} "
                    f"(–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})"
                )
                return result
                
            except ValidationError as e:
                last_error = e
                logger.warning(
                    f"‚ö†Ô∏è Validation failed (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries + 1}): {e}"
                )
                
            except json.JSONDecodeError as e:
                last_error = e
                logger.warning(
                    f"‚ö†Ô∏è JSON decode failed (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries + 1}): {e}"
                )
                
            except Exception as e:
                last_error = e
                logger.error(f"‚ùå Structured output error: {e}")
                if attempt >= retries:
                    break
        
        raise StructuredOutputError(
            f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π {response_model.__name__} "
            f"–ø–æ—Å–ª–µ {retries + 1} –ø–æ–ø—ã—Ç–æ–∫: {last_error}"
        )
    
    def _extract_json(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞.
        
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–∏ –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON.
        –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–µ–∫.
        
        Args:
            text: –¢–µ–∫—Å—Ç —Å JSON
            
        Returns:
            –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π JSON string
        """
        text = text.strip()
        
        # –£–±–∏—Ä–∞–µ–º markdown –±–ª–æ–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()
        
        # –ò—â–µ–º JSON –æ–±—ä–µ–∫—Ç –∏–ª–∏ –º–∞—Å—Å–∏–≤
        start = text.find('{')
        is_array = False
        if start == -1:
            start = text.find('[')
            is_array = True
        
        if start == -1:
            return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å, –ø—É—Å—Ç—å JSON –ø–∞—Ä—Å–µ—Ä —Ä–∞–∑–±–µ—Ä—ë—Ç—Å—è
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–µ–∫
        # –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
        def find_matching_brace(text: str, start_pos: int, open_char: str, close_char: str) -> int:
            """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ–∑–∏—Ü–∏—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏.
            
            Args:
                text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
                start_pos: –ü–æ–∑–∏—Ü–∏—è –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏
                open_char: –°–∏–º–≤–æ–ª –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏ ('{' –∏–ª–∏ '[')
                close_char: –°–∏–º–≤–æ–ª –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏ ('}' –∏–ª–∏ ']')
                
            Returns:
                –ü–æ–∑–∏—Ü–∏—è –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏ –∏–ª–∏ -1 –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
            """
            stack = []
            in_string = False
            escape_next = False
            
            for i in range(start_pos, len(text)):
                char = text[i]
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ —Å—Ç—Ä–æ–∫–∞—Ö
                if escape_next:
                    escape_next = False
                    continue
                
                if char == '\\':
                    escape_next = True
                    continue
                
                # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ (JSON –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–∫–æ–±–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫)
                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue
                
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–∫–æ–±–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫
                if in_string:
                    continue
                
                # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Å–∫–æ–±–∫–∏
                if char == open_char:
                    stack.append(open_char)
                elif char == close_char:
                    if stack:
                        stack.pop()
                        if not stack:
                            return i
            
            return -1
        
        # –ù–∞—Ö–æ–¥–∏–º –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
        open_char = '[' if is_array else '{'
        close_char = ']' if is_array else '}'
        end = find_matching_brace(text, start, open_char, close_char)
        
        if end == -1 or end < start:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞
            return text[start:]
        
        return text[start:end + 1]
    
    async def generate_structured_async(
        self,
        prompt: str,
        response_model: Type[T],
        num_predict: int = 1024,
        retries: int = 2
    ) -> T:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è generate_structured.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            response_model: Pydantic –º–æ–¥–µ–ª—å
            num_predict: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤
            
        Returns:
            –ü—Ä–æ–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Pydantic –æ–±—ä–µ–∫—Ç
        """
        return await asyncio.to_thread(
            self.generate_structured,
            prompt,
            response_model,
            num_predict,
            retries
        )
    
    # === STREAMING –ú–ï–¢–û–î–´ ===
    # Real-time —Å—Ç—Ä–∏–º–∏–Ω–≥ –¥–ª—è UI: thinking –±–ª–æ–∫–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –∏ —Ç.–¥.
    
    async def generate_stream(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: int = 4096,
        format: Optional[str] = None,
        **kwargs: Any
    ):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å—Ç—Ä–∏–º–∏–Ω–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Ollama streaming API –¥–ª—è real-time –æ—Ç–¥–∞—á–∏ —á–∞–Ω–∫–æ–≤.
        –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å <think> –±–ª–æ–∫–∏ –∏ –∫–æ–¥ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            format: –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ("json" –¥–ª—è JSON)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Yields:
            StreamChunk —Å –¥–∞–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞
            
        Example:
            async for chunk in llm.generate_stream(prompt):
                if chunk.is_thinking:
                    yield thinking_event(chunk.content)
                else:
                    yield code_event(chunk.content)
        """
        temp = temperature if temperature is not None else self.temperature
        tp = top_p if top_p is not None else self.top_p
        
        options: Dict[str, Any] = {
            "temperature": temp,
            "top_p": tp,
            "num_predict": num_predict
        }
        options.update(kwargs.get("options", {}))
        
        generate_kwargs = {
            "model": self.model,
            "prompt": prompt,
            "options": options,
            "stream": True  # –í–∫–ª—é—á–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥
        }
        
        if format:
            generate_kwargs["format"] = format
        
        for k, v in kwargs.items():
            if k not in ("options", "format"):
                generate_kwargs[k] = v
        
        full_response = ""
        in_thinking = False
        
        try:
            # Ollama streaming API
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
            import queue
            import threading
            
            chunk_queue: queue.Queue = queue.Queue()
            error_holder: list = []
            
            def stream_worker():
                try:
                    for chunk in ollama.generate(**generate_kwargs):
                        chunk_queue.put(chunk)
                    chunk_queue.put(None)  # –°–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                except Exception as e:
                    error_holder.append(e)
                    chunk_queue.put(None)
            
            thread = threading.Thread(target=stream_worker, daemon=True)
            thread.start()
            
            wait_count = 0
            last_log_time = asyncio.get_event_loop().time()
            
            stream_start_time = time.time()
            max_stream_time = self.timeout * 2  # –ú–∞–∫—Å–∏–º—É–º 2x timeout –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
            
            while True:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π timeout —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
                elapsed_stream = time.time() - stream_start_time
                if elapsed_stream > max_stream_time:
                    logger.error(
                        f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω –æ–±—â–∏–π timeout —Å—Ç—Ä–∏–º–∏–Ω–≥–∞: {elapsed_stream:.1f}—Å "
                        f"(–º–∞–∫—Å–∏–º—É–º: {max_stream_time}—Å)"
                    )
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –≤ —Å–ø–∏—Å–æ–∫ (–µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç, —Å–æ–∑–¥–∞—ë–º –µ–≥–æ)
                    if not error_holder:
                        error_holder.append(LLMTimeoutError(
                            f"–ü—Ä–µ–≤—ã—à–µ–Ω –æ–±—â–∏–π timeout —Å—Ç—Ä–∏–º–∏–Ω–≥–∞: {elapsed_stream:.1f}—Å"
                        ))
                    else:
                        error_holder[0] = LLMTimeoutError(
                            f"–ü—Ä–µ–≤—ã—à–µ–Ω –æ–±—â–∏–π timeout —Å—Ç—Ä–∏–º–∏–Ω–≥–∞: {elapsed_stream:.1f}—Å"
                        )
                    chunk_queue.put(None)  # –°–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    break
                
                try:
                    # –ù–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–µ –æ–∂–∏–¥–∞–Ω–∏–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                    chunk = await asyncio.get_event_loop().run_in_executor(
                        None,
                        lambda: chunk_queue.get(timeout=0.5)
                    )
                    wait_count = 0  # Reset on successful get
                except queue.Empty:
                    wait_count += 1
                    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥ –æ–∂–∏–¥–∞–Ω–∏—è
                    current_time = asyncio.get_event_loop().time()
                    if current_time - last_log_time > 10:
                        elapsed_total = time.time() - stream_start_time
                        logger.info(
                            f"‚è≥ –û–∂–∏–¥–∞—é –æ—Ç–≤–µ—Ç –æ—Ç LLM... "
                            f"(–æ–∂–∏–¥–∞–Ω–∏–µ: {wait_count * 0.5:.0f}—Å, –≤—Å–µ–≥–æ: {elapsed_total:.0f}—Å)"
                        )
                        last_log_time = current_time
                    continue
                
                if chunk is None:
                    # –°—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω
                    if error_holder and error_holder[0]:
                        elapsed_total = time.time() - stream_start_time
                        logger.error(
                            f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –ø–æ—Å–ª–µ {elapsed_total:.1f}—Å: {error_holder[0]}"
                        )
                        raise error_holder[0]
                    break
                
                content = chunk.get("response", "")
                is_done = chunk.get("done", False)
                
                if content:
                    full_response += content
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—Ö–æ–¥–∏–º—Å—è –ª–∏ –≤–Ω—É—Ç—Ä–∏ <think> –±–ª–æ–∫–∞
                    # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–µ/–∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Ç–µ–≥–∏
                    think_opens = full_response.lower().count("<think>")
                    think_closes = full_response.lower().count("</think>")
                    in_thinking = think_opens > think_closes
                    
                    yield StreamChunk(
                        content=content,
                        is_thinking=in_thinking,
                        is_done=is_done,
                        full_response=full_response
                    )
                
                if is_done:
                    break
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫
            if full_response:
                yield StreamChunk(
                    content="",
                    is_thinking=False,
                    is_done=True,
                    full_response=full_response
                )
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ LLM: {e}", error=e)
            # Yield –ø—É—Å—Ç–æ–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            yield StreamChunk(
                content="",
                is_thinking=False,
                is_done=True,
                full_response=full_response
            )
    
    # === ASYNC –ú–ï–¢–û–î–´ ===
    # –ò—Å–ø–æ–ª—å–∑—É—é—Ç asyncio.to_thread() –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∫–æ–¥–æ–º
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop FastAPI –ø—Ä–∏ LLM –∑–∞–ø—Ä–æ—Å–∞—Ö
    
    async def generate_async(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: int = 4096,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ ollama.generate()
        –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è event loop.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        return await asyncio.to_thread(
            self.generate,
            prompt,
            temperature,
            top_p,
            num_predict,
            **kwargs
        )
    
    async def chat_async(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ ollama.chat()
        –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è event loop.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        return await asyncio.to_thread(
            self.chat,
            messages,
            temperature,
            top_p,
            **kwargs
        )


class AsyncLocalLLM:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama —á–µ—Ä–µ–∑ httpx.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OllamaConnectionPool –¥–ª—è HTTP/2 –∏ connection pooling.
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è production –∏ –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞.
    
    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¢—Ä–µ–±—É–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—É–ª–∞ —á–µ—Ä–µ–∑ get_ollama_pool().
    """
    
    def __init__(
        self,
        model: str,
        temperature: float = 0.25,
        top_p: float = 0.9,
        num_predict: int = 4096
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AsyncLocalLLM.
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        self.num_predict = num_predict
    
    async def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: Optional[int] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ httpx.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        from infrastructure.connection_pool import get_ollama_pool
        
        pool = await get_ollama_pool()
        
        options = {
            "temperature": temperature if temperature is not None else self.temperature,
            "top_p": top_p if top_p is not None else self.top_p,
            "num_predict": num_predict if num_predict is not None else self.num_predict
        }
        
        try:
            result = await pool.generate(
                model=self.model,
                prompt=prompt,
                options=options
            )
            return result.strip() if result else ""
        except Exception as e:
            logger.error(f"‚ùå AsyncLocalLLM –æ—à–∏–±–∫–∞: {e}", error=e)
            return ""
    
    async def chat(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞ —á–µ—Ä–µ–∑ httpx.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        from infrastructure.connection_pool import get_ollama_pool
        
        pool = await get_ollama_pool()
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature if temperature is not None else self.temperature,
                "top_p": top_p if top_p is not None else self.top_p
            }
        }
        
        try:
            response = await pool.post("/api/chat", json=payload)
            data = response.json()
            return data.get("message", {}).get("content", "").strip()
        except Exception as e:
            logger.error(f"‚ùå AsyncLocalLLM chat –æ—à–∏–±–∫–∞: {e}", error=e)
            return ""


def create_llm_for_stage(
    stage: str,
    model: str | None = None,
    temperature: float = 0.25,
    top_p: float = 0.9
) -> LocalLLM:
    """–°–æ–∑–¥–∞—ë—Ç LocalLLM —Å —Ç–∞–π–º–∞—É—Ç–æ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º —ç—Ç–∞–ø—É workflow.
    
    Args:
        stage: –ù–∞–∑–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–∞ (intent, planning, coding, etc.)
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama (None = default)
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
        
    Returns:
        LocalLLM —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º –¥–ª—è —ç—Ç–∞–ø–∞
    """
    from utils.config import get_config
    config = get_config()
    timeout = config.get_stage_timeout(stage)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
    resolved_model = model or "qwen2.5-coder:7b"
    
    return LocalLLM(
        model=resolved_model,
        temperature=temperature,
        top_p=top_p,
        timeout=timeout
    )
