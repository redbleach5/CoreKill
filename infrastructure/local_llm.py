"""–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ LLM —á–µ—Ä–µ–∑ Ollama.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã:
- –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π (generate) ‚Äî –¥–ª—è CLI –∏ –ø—Ä–æ—Å—Ç—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π (generate_async) ‚Äî –¥–ª—è FastAPI –∏ –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —É–¥–∞–ª—ë–Ω–Ω–æ–≥–æ Ollama:
- –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ config.toml [ollama] —Å–µ–∫—Ü–∏—é
- –î–ª—è —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å–µ—Ç—è–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Tailscale (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –†–§)

Structured Output:
- generate_structured() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Pydantic –º–æ–¥–µ–ª–∏ —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç format="json" Ollama –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ JSON

Reasoning Models:
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ DeepSeek-R1, QwQ —Å <think> –±–ª–æ–∫–∞–º–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ reasoning —á–µ—Ä–µ–∑ reasoning_utils

–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º,
–∞ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å httpx —á–µ—Ä–µ–∑ OllamaConnectionPool –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""
import asyncio
import json
import os
import ollama
from typing import Optional, Dict, Any, Type, TypeVar
import time
import concurrent.futures

from pydantic import BaseModel, ValidationError

from utils.logger import get_logger


logger = get_logger()


def _configure_ollama_host() -> None:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ö–æ—Å—Ç Ollama –∏–∑ config.toml.
    
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OLLAMA_HOST –∫–æ—Ç–æ—Ä—É—é
    –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ollama Python SDK.
    """
    # –ù–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –µ—Å–ª–∏ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤—Ä—É—á–Ω—É—é
    if os.environ.get("OLLAMA_HOST"):
        return
    
    try:
        from utils.config import get_config
        config = get_config()
        host = config.ollama_host
        
        if host and host != "http://localhost:11434":
            os.environ["OLLAMA_HOST"] = host
            logger.info(f"üåê Ollama —Ö–æ—Å—Ç: {host}")
    except Exception:
        pass  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç


# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
_configure_ollama_host()


class LLMTimeoutError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è —Ç–∞–π–º–∞—É—Ç–∞ LLM –∑–∞–ø—Ä–æ—Å–∞."""
    pass


class StructuredOutputError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ structured output."""
    pass


# TypeVar –¥–ª—è generic generate_structured
T = TypeVar('T', bound=BaseModel)


class LocalLLM:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama API.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç retry —Å exponential backoff, –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫.
    """

    # –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è exponential backoff (—Å–µ–∫—É–Ω–¥—ã)
    BASE_RETRY_DELAY = 1.0
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É retry
    MAX_RETRY_DELAY = 30.0

    def __init__(
        self,
        model: str,
        temperature: float = 0.25,
        top_p: float = 0.9,
        timeout: int = 120,
        max_retries: int = 3
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LocalLLM.
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.15-0.35 –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º)
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 120—Å)
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        """
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        self.timeout = timeout
        self.max_retries = max_retries
    
    def _calculate_backoff(self, attempt: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è exponential backoff.
        
        Args:
            attempt: –ù–æ–º–µ—Ä –ø–æ–ø—ã—Ç–∫–∏ (–Ω–∞—á–∏–Ω–∞—è —Å 0)
            
        Returns:
            –ó–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        delay = self.BASE_RETRY_DELAY * (2 ** attempt)
        return min(delay, self.MAX_RETRY_DELAY)

    def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: int = 4096,
        format: Optional[str] = None,
        **kwargs: Any
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            format: –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ("json" –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ JSON)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ollama.generate
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        temp = temperature if temperature is not None else self.temperature
        tp = top_p if top_p is not None else self.top_p
        
        options: Dict[str, Any] = {
            "temperature": temp,
            "top_p": tp,
            "num_predict": num_predict
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        options.update(kwargs.get("options", {}))
        
        last_error: Optional[Exception] = None
        
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á (intent, planning) —É–º–µ–Ω—å—à–∞–µ–º num_predict –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if num_predict > 1024 and len(prompt) < 500:
            options["num_predict"] = min(512, num_predict // 2)
        
        for attempt in range(self.max_retries + 1):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω (—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–µ)
                if attempt == 0:
                    try:
                        ollama.list()
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ —Å–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω: {e}")
                        return ""
                
                start_time = time.time()
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è ollama.generate
                generate_kwargs = {
                    "model": self.model,
                    "prompt": prompt,
                    "options": options,
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º format –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω (–¥–ª—è JSON output)
                if format:
                    generate_kwargs["format"] = format
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ kwargs (–∫—Ä–æ–º–µ options –∏ format)
                for k, v in kwargs.items():
                    if k not in ("options", "format"):
                        generate_kwargs[k] = v
                
                # –í—ã–∑–æ–≤ —Å timeout —á–µ—Ä–µ–∑ ThreadPoolExecutor (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ª—é–±–æ–º –ø–æ—Ç–æ–∫–µ)
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(
                        ollama.generate,
                        **generate_kwargs
                    )
                    
                    try:
                        response = future.result(timeout=self.timeout)
                    except concurrent.futures.TimeoutError:
                        elapsed = time.time() - start_time
                        logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç LLM –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                        raise LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                
                elapsed = time.time() - start_time
                result = response.get("response", "").strip()
                
                if result:
                    logger.debug(f"‚úÖ LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.1f}—Å ({len(result)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                    
            except (LLMTimeoutError, concurrent.futures.TimeoutError):
                last_error = LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å...")
                    time.sleep(backoff)
                    continue
                    
            except Exception as e:
                last_error = e
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {e}")
                    time.sleep(backoff)
                    continue
                else:
                    break
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
        error_msg = f"–û—à–∏–±–∫–∞ Ollama –ø–æ—Å–ª–µ {self.max_retries + 1} –ø–æ–ø—ã—Ç–æ–∫: {last_error}"
        logger.error(f"‚ùå {error_msg}", error=last_error)
        return ""

    def chat(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user", "content": "..."}]
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        temp = temperature if temperature is not None else self.temperature
        tp = top_p if top_p is not None else self.top_p
        
        options: Dict[str, Any] = {
            "temperature": temp,
            "top_p": tp
        }
        options.update(kwargs.get("options", {}))
        
        last_error: Optional[Exception] = None
        
        for attempt in range(self.max_retries + 1):
            try:
                start_time = time.time()
                
                # –í—ã–∑–æ–≤ —Å timeout —á–µ—Ä–µ–∑ ThreadPoolExecutor
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(
                        ollama.chat,
                        model=self.model,
                        messages=messages,
                        options=options,
                        **{k: v for k, v in kwargs.items() if k != "options"}
                    )
                    
                    try:
                        response = future.result(timeout=self.timeout)
                    except concurrent.futures.TimeoutError:
                        elapsed = time.time() - start_time
                        logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç chat –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                        raise LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                
                result = response.get("message", {}).get("content", "").strip()
                if result:
                    return result
                    
            except (LLMTimeoutError, concurrent.futures.TimeoutError):
                last_error = LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Chat retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å...")
                    time.sleep(backoff)
                    continue
                    
            except Exception as e:
                last_error = e
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Chat retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {e}")
                    time.sleep(backoff)
                    continue
                else:
                    break
        
        error_msg = f"–û—à–∏–±–∫–∞ Ollama chat –ø–æ—Å–ª–µ {self.max_retries + 1} –ø–æ–ø—ã—Ç–æ–∫: {last_error}"
        logger.error(f"‚ùå {error_msg}", error=last_error)
        return ""
    
    # === STRUCTURED OUTPUT ===
    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Pydantic –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–æ–≤
    
    def generate_structured(
        self,
        prompt: str,
        response_model: Type[T],
        num_predict: int = 1024,
        retries: int = 2
    ) -> T:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å Pydantic –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç format="json" Ollama –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ JSON —Ñ–æ—Ä–º–∞—Ç–∞,
        –∑–∞—Ç–µ–º –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç —á–µ—Ä–µ–∑ Pydantic –º–æ–¥–µ–ª—å.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            response_model: Pydantic –º–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
            num_predict: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            –ü—Ä–æ–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Pydantic –æ–±—ä–µ–∫—Ç
            
        Raises:
            StructuredOutputError: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç
            
        Example:
            from models import IntentResponse
            
            response = llm.generate_structured(
                "Classify: –Ω–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é",
                IntentResponse
            )
            print(response.intent)  # "create"
        """
        schema = response_model.model_json_schema()
        schema_str = json.dumps(schema, indent=2)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ —Ñ–æ—Ä–º–∞—Ç—É –≤ –ø—Ä–æ–º–ø—Ç
        enhanced_prompt = f"""{prompt}

IMPORTANT: Return response as valid JSON matching this schema:
{schema_str}

JSON:"""
        
        last_error: Optional[Exception] = None
        
        for attempt in range(retries + 1):
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å format="json"
                response = self.generate(
                    prompt=enhanced_prompt,
                    num_predict=num_predict,
                    format="json"  # Ollama –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON
                )
                
                if not response:
                    raise StructuredOutputError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM")
                
                # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                # –ò–Ω–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON
                json_str = self._extract_json(response)
                
                # Pydantic –≤–∞–ª–∏–¥–∞—Ü–∏—è
                result = response_model.model_validate_json(json_str)
                
                logger.debug(
                    f"‚úÖ Structured output —É—Å–ø–µ—à–Ω–æ: {response_model.__name__} "
                    f"(–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})"
                )
                return result
                
            except ValidationError as e:
                last_error = e
                logger.warning(
                    f"‚ö†Ô∏è Validation failed (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries + 1}): {e}"
                )
                
            except json.JSONDecodeError as e:
                last_error = e
                logger.warning(
                    f"‚ö†Ô∏è JSON decode failed (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries + 1}): {e}"
                )
                
            except Exception as e:
                last_error = e
                logger.error(f"‚ùå Structured output error: {e}")
                if attempt >= retries:
                    break
        
        raise StructuredOutputError(
            f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π {response_model.__name__} "
            f"–ø–æ—Å–ª–µ {retries + 1} –ø–æ–ø—ã—Ç–æ–∫: {last_error}"
        )
    
    def _extract_json(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞.
        
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–∏ –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON.
        
        Args:
            text: –¢–µ–∫—Å—Ç —Å JSON
            
        Returns:
            –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π JSON string
        """
        text = text.strip()
        
        # –£–±–∏—Ä–∞–µ–º markdown –±–ª–æ–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()
        
        # –ò—â–µ–º JSON –æ–±—ä–µ–∫—Ç
        start = text.find('{')
        if start == -1:
            # –ú–æ–∂–µ—Ç –±—ã—Ç—å –º–∞—Å—Å–∏–≤
            start = text.find('[')
        
        if start == -1:
            return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å, –ø—É—Å—Ç—å JSON –ø–∞—Ä—Å–µ—Ä —Ä–∞–∑–±–µ—Ä—ë—Ç—Å—è
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
        end = text.rfind('}')
        if start >= 0 and text[start] == '[':
            end = text.rfind(']')
        
        if end == -1 or end < start:
            return text[start:]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç –Ω–∞—á–∞–ª–∞ JSON –¥–æ –∫–æ–Ω—Ü–∞
        
        return text[start:end + 1]
    
    async def generate_structured_async(
        self,
        prompt: str,
        response_model: Type[T],
        num_predict: int = 1024,
        retries: int = 2
    ) -> T:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è generate_structured.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            response_model: Pydantic –º–æ–¥–µ–ª—å
            num_predict: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤
            
        Returns:
            –ü—Ä–æ–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Pydantic –æ–±—ä–µ–∫—Ç
        """
        return await asyncio.to_thread(
            self.generate_structured,
            prompt,
            response_model,
            num_predict,
            retries
        )
    
    # === ASYNC –ú–ï–¢–û–î–´ ===
    # –ò—Å–ø–æ–ª—å–∑—É—é—Ç asyncio.to_thread() –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∫–æ–¥–æ–º
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop FastAPI –ø—Ä–∏ LLM –∑–∞–ø—Ä–æ—Å–∞—Ö
    
    async def generate_async(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: int = 4096,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ ollama.generate()
        –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è event loop.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        return await asyncio.to_thread(
            self.generate,
            prompt,
            temperature,
            top_p,
            num_predict,
            **kwargs
        )
    
    async def chat_async(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ ollama.chat()
        –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è event loop.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        return await asyncio.to_thread(
            self.chat,
            messages,
            temperature,
            top_p,
            **kwargs
        )


class AsyncLocalLLM:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama —á–µ—Ä–µ–∑ httpx.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OllamaConnectionPool –¥–ª—è HTTP/2 –∏ connection pooling.
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è production –∏ –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞.
    
    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¢—Ä–µ–±—É–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—É–ª–∞ —á–µ—Ä–µ–∑ get_ollama_pool().
    """
    
    def __init__(
        self,
        model: str,
        temperature: float = 0.25,
        top_p: float = 0.9,
        num_predict: int = 4096
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AsyncLocalLLM.
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        self.num_predict = num_predict
    
    async def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: Optional[int] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ httpx.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        from infrastructure.connection_pool import get_ollama_pool
        
        pool = await get_ollama_pool()
        
        options = {
            "temperature": temperature if temperature is not None else self.temperature,
            "top_p": top_p if top_p is not None else self.top_p,
            "num_predict": num_predict if num_predict is not None else self.num_predict
        }
        
        try:
            result = await pool.generate(
                model=self.model,
                prompt=prompt,
                options=options
            )
            return result.strip() if result else ""
        except Exception as e:
            logger.error(f"‚ùå AsyncLocalLLM –æ—à–∏–±–∫–∞: {e}", error=e)
            return ""
    
    async def chat(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞ —á–µ—Ä–µ–∑ httpx.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        from infrastructure.connection_pool import get_ollama_pool
        
        pool = await get_ollama_pool()
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature if temperature is not None else self.temperature,
                "top_p": top_p if top_p is not None else self.top_p
            }
        }
        
        try:
            response = await pool.post("/api/chat", json=payload)
            data = response.json()
            return data.get("message", {}).get("content", "").strip()
        except Exception as e:
            logger.error(f"‚ùå AsyncLocalLLM chat –æ—à–∏–±–∫–∞: {e}", error=e)
            return ""


def create_llm_for_stage(
    stage: str,
    model: str,
    temperature: float = 0.25,
    top_p: float = 0.9
) -> LocalLLM:
    """–°–æ–∑–¥–∞—ë—Ç LocalLLM —Å —Ç–∞–π–º–∞—É—Ç–æ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º —ç—Ç–∞–ø—É workflow.
    
    Args:
        stage: –ù–∞–∑–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–∞ (intent, planning, coding, etc.)
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
        
    Returns:
        LocalLLM —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º –¥–ª—è —ç—Ç–∞–ø–∞
    """
    from utils.config import get_config
    config = get_config()
    timeout = config.get_stage_timeout(stage)
    
    return LocalLLM(
        model=model,
        temperature=temperature,
        top_p=top_p,
        timeout=timeout
    )
