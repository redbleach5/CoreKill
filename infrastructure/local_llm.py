"""–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ LLM —á–µ—Ä–µ–∑ Ollama.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã:
- –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π (generate) ‚Äî –¥–ª—è CLI –∏ –ø—Ä–æ—Å—Ç—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π (generate_async) ‚Äî –¥–ª—è FastAPI –∏ –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞

–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º,
–∞ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å httpx —á–µ—Ä–µ–∑ OllamaConnectionPool –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""
import asyncio
import ollama
from typing import Optional, Dict, Any
import time
import concurrent.futures

from utils.logger import get_logger


logger = get_logger()


class LLMTimeoutError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è —Ç–∞–π–º–∞—É—Ç–∞ LLM –∑–∞–ø—Ä–æ—Å–∞."""
    pass


class LocalLLM:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama API.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç retry —Å exponential backoff, –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫.
    """

    # –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è exponential backoff (—Å–µ–∫—É–Ω–¥—ã)
    BASE_RETRY_DELAY = 1.0
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É retry
    MAX_RETRY_DELAY = 30.0

    def __init__(
        self,
        model: str,
        temperature: float = 0.25,
        top_p: float = 0.9,
        timeout: int = 120,
        max_retries: int = 3
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LocalLLM.
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.15-0.35 –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º)
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 120—Å)
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        """
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        self.timeout = timeout
        self.max_retries = max_retries
    
    def _calculate_backoff(self, attempt: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è exponential backoff.
        
        Args:
            attempt: –ù–æ–º–µ—Ä –ø–æ–ø—ã—Ç–∫–∏ (–Ω–∞—á–∏–Ω–∞—è —Å 0)
            
        Returns:
            –ó–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        delay = self.BASE_RETRY_DELAY * (2 ** attempt)
        return min(delay, self.MAX_RETRY_DELAY)

    def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: int = 4096,
        **kwargs: Any
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ollama.generate
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        temp = temperature if temperature is not None else self.temperature
        tp = top_p if top_p is not None else self.top_p
        
        options: Dict[str, Any] = {
            "temperature": temp,
            "top_p": tp,
            "num_predict": num_predict
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        options.update(kwargs.get("options", {}))
        
        last_error: Optional[Exception] = None
        
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á (intent, planning) —É–º–µ–Ω—å—à–∞–µ–º num_predict –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if num_predict > 1024 and len(prompt) < 500:
            options["num_predict"] = min(512, num_predict // 2)
        
        for attempt in range(self.max_retries + 1):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω (—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–µ)
                if attempt == 0:
                    try:
                        ollama.list()
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ —Å–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω: {e}")
                        return ""
                
                start_time = time.time()
                
                # –í—ã–∑–æ–≤ —Å timeout —á–µ—Ä–µ–∑ ThreadPoolExecutor (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ª—é–±–æ–º –ø–æ—Ç–æ–∫–µ)
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(
                        ollama.generate,
                        model=self.model,
                        prompt=prompt,
                        options=options,
                        **{k: v for k, v in kwargs.items() if k != "options"}
                    )
                    
                    try:
                        response = future.result(timeout=self.timeout)
                    except concurrent.futures.TimeoutError:
                        elapsed = time.time() - start_time
                        logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç LLM –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                        raise LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                
                elapsed = time.time() - start_time
                result = response.get("response", "").strip()
                
                if result:
                    logger.debug(f"‚úÖ LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.1f}—Å ({len(result)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                    
            except (LLMTimeoutError, concurrent.futures.TimeoutError):
                last_error = LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å...")
                    time.sleep(backoff)
                    continue
                    
            except Exception as e:
                last_error = e
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {e}")
                    time.sleep(backoff)
                    continue
                else:
                    break
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
        error_msg = f"–û—à–∏–±–∫–∞ Ollama –ø–æ—Å–ª–µ {self.max_retries + 1} –ø–æ–ø—ã—Ç–æ–∫: {last_error}"
        logger.error(f"‚ùå {error_msg}", error=last_error)
        return ""

    def chat(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user", "content": "..."}]
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        temp = temperature if temperature is not None else self.temperature
        tp = top_p if top_p is not None else self.top_p
        
        options: Dict[str, Any] = {
            "temperature": temp,
            "top_p": tp
        }
        options.update(kwargs.get("options", {}))
        
        last_error: Optional[Exception] = None
        
        for attempt in range(self.max_retries + 1):
            try:
                start_time = time.time()
                
                # –í—ã–∑–æ–≤ —Å timeout —á–µ—Ä–µ–∑ ThreadPoolExecutor
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(
                        ollama.chat,
                        model=self.model,
                        messages=messages,
                        options=options,
                        **{k: v for k, v in kwargs.items() if k != "options"}
                    )
                    
                    try:
                        response = future.result(timeout=self.timeout)
                    except concurrent.futures.TimeoutError:
                        elapsed = time.time() - start_time
                        logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç chat –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ {elapsed:.1f}—Å")
                        raise LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                
                result = response.get("message", {}).get("content", "").strip()
                if result:
                    return result
                    
            except (LLMTimeoutError, concurrent.futures.TimeoutError):
                last_error = LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç {self.timeout}—Å")
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Chat retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å...")
                    time.sleep(backoff)
                    continue
                    
            except Exception as e:
                last_error = e
                backoff = self._calculate_backoff(attempt)
                if attempt < self.max_retries:
                    logger.info(f"üîÑ Chat retry {attempt + 1}/{self.max_retries} —á–µ—Ä–µ–∑ {backoff:.1f}—Å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {e}")
                    time.sleep(backoff)
                    continue
                else:
                    break
        
        error_msg = f"–û—à–∏–±–∫–∞ Ollama chat –ø–æ—Å–ª–µ {self.max_retries + 1} –ø–æ–ø—ã—Ç–æ–∫: {last_error}"
        logger.error(f"‚ùå {error_msg}", error=last_error)
        return ""
    
    # === ASYNC –ú–ï–¢–û–î–´ ===
    # –ò—Å–ø–æ–ª—å–∑—É—é—Ç asyncio.to_thread() –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∫–æ–¥–æ–º
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop FastAPI –ø—Ä–∏ LLM –∑–∞–ø—Ä–æ—Å–∞—Ö
    
    async def generate_async(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: int = 4096,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ ollama.generate()
        –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è event loop.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        return await asyncio.to_thread(
            self.generate,
            prompt,
            temperature,
            top_p,
            num_predict,
            **kwargs
        )
    
    async def chat_async(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio.to_thread() –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ ollama.chat()
        –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è event loop.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        return await asyncio.to_thread(
            self.chat,
            messages,
            temperature,
            top_p,
            **kwargs
        )


class AsyncLocalLLM:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama —á–µ—Ä–µ–∑ httpx.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OllamaConnectionPool –¥–ª—è HTTP/2 –∏ connection pooling.
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è production –∏ –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞.
    
    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¢—Ä–µ–±—É–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—É–ª–∞ —á–µ—Ä–µ–∑ get_ollama_pool().
    """
    
    def __init__(
        self,
        model: str,
        temperature: float = 0.25,
        top_p: float = 0.9,
        num_predict: int = 4096
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AsyncLocalLLM.
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        self.num_predict = num_predict
    
    async def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: Optional[int] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ httpx.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            num_predict: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        from infrastructure.connection_pool import get_ollama_pool
        
        pool = await get_ollama_pool()
        
        options = {
            "temperature": temperature if temperature is not None else self.temperature,
            "top_p": top_p if top_p is not None else self.top_p,
            "num_predict": num_predict if num_predict is not None else self.num_predict
        }
        
        try:
            result = await pool.generate(
                model=self.model,
                prompt=prompt,
                options=options
            )
            return result.strip() if result else ""
        except Exception as e:
            logger.error(f"‚ùå AsyncLocalLLM –æ—à–∏–±–∫–∞: {e}", error=e)
            return ""
    
    async def chat(
        self,
        messages: list[Dict[str, str]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs: Any
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞ —á–µ—Ä–µ–∑ httpx.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        from infrastructure.connection_pool import get_ollama_pool
        
        pool = await get_ollama_pool()
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature if temperature is not None else self.temperature,
                "top_p": top_p if top_p is not None else self.top_p
            }
        }
        
        try:
            response = await pool.post("/api/chat", json=payload)
            data = response.json()
            return data.get("message", {}).get("content", "").strip()
        except Exception as e:
            logger.error(f"‚ùå AsyncLocalLLM chat –æ—à–∏–±–∫–∞: {e}", error=e)
            return ""


def create_llm_for_stage(
    stage: str,
    model: str,
    temperature: float = 0.25,
    top_p: float = 0.9
) -> LocalLLM:
    """–°–æ–∑–¥–∞—ë—Ç LocalLLM —Å —Ç–∞–π–º–∞—É—Ç–æ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º —ç—Ç–∞–ø—É workflow.
    
    Args:
        stage: –ù–∞–∑–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–∞ (intent, planning, coding, etc.)
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        top_p: –ü–∞—Ä–∞–º–µ—Ç—Ä top_p
        
    Returns:
        LocalLLM —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º –¥–ª—è —ç—Ç–∞–ø–∞
    """
    from utils.config import get_config
    config = get_config()
    timeout = config.get_stage_timeout(stage)
    
    return LocalLLM(
        model=model,
        temperature=temperature,
        top_p=top_p,
        timeout=timeout
    )
