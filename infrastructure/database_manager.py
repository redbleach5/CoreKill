"""–°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞.

–£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º–∏ –ë–î, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã:
- ChromaDB (RAG, –ø–∞–º—è—Ç—å –∑–∞–¥–∞—á)
- SQLite (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
- JSON —Ñ–∞–π–ª—ã (–¥–∏–∞–ª–æ–≥–∏)
- –ö—ç—à–∏

–§—É–Ω–∫—Ü–∏–∏:
- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
- –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
- –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–∑–º–µ—Ä–∞
"""
import os
import shutil
import json
import sqlite3
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
import hashlib

from utils.logger import get_logger
from utils.config import get_config

logger = get_logger()


@dataclass
class DatabaseInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
    name: str
    type: str  # "chromadb", "sqlite", "json", "cache"
    path: Path
    size_bytes: int
    collections: Optional[List[str]] = None  # –î–ª—è ChromaDB
    last_modified: Optional[datetime] = None
    record_count: Optional[int] = None


class DatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ–º–∏ –ë–î —Å–∏—Å—Ç–µ–º—ã."""
    
    def __init__(self, base_dir: Optional[Path] = None) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ë–î.
        
        Args:
            base_dir: –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–µ–∫—É—â–∞—è)
        """
        self.base_dir = base_dir or Path.cwd()
        self.config = get_config()
        
        # –ü—É—Ç–∏ –∫ –ë–î
        self.chromadb_dir = self.base_dir / self.config.rag_persist_directory
        self.conversations_dir = self.base_dir / "output" / "conversations"
        self.context_cache_dir = self.base_dir / self.config.context_engine_cache_directory
        self.learning_db_path = self.base_dir / "output" / "learning.db"
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –±—ç–∫–∞–ø–æ–≤
        self.backup_dir = self.base_dir / "output" / "backups"
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def discover_databases(self) -> List[DatabaseInfo]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –≤—Å–µ –ë–î –≤ —Å–∏—Å—Ç–µ–º–µ.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ë–î
        """
        databases: List[DatabaseInfo] = []
        
        # ChromaDB
        if self.chromadb_dir.exists():
            chroma_dbs = self._discover_chromadb()
            databases.extend(chroma_dbs)
        
        # SQLite
        sqlite_dbs = self._discover_sqlite()
        databases.extend(sqlite_dbs)
        
        # JSON —Ñ–∞–π–ª—ã (–¥–∏–∞–ª–æ–≥–∏)
        json_dbs = self._discover_json_conversations()
        databases.extend(json_dbs)
        
        # –ö—ç—à–∏
        cache_dbs = self._discover_caches()
        databases.extend(cache_dbs)
        
        return databases
    
    def _discover_chromadb(self) -> List[DatabaseInfo]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        databases: List[DatabaseInfo] = []
        
        if not self.chromadb_dir.exists():
            return databases
        
        try:
            import chromadb
            from chromadb.config import Settings
            
            client = chromadb.PersistentClient(
                path=str(self.chromadb_dir),
                settings=Settings(anonymized_telemetry=False)
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collections = client.list_collections()
            
            for collection in collections:
                collection_name = collection.name
                collection_path = self.chromadb_dir / collection_name
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                size = self._get_directory_size(collection_path) if collection_path.exists() else 0
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
                try:
                    count = collection.count()
                except Exception:
                    count = None
                
                databases.append(DatabaseInfo(
                    name=f"chromadb:{collection_name}",
                    type="chromadb",
                    path=collection_path,
                    size_bytes=size,
                    collections=[collection_name],
                    record_count=count
                ))
            
            # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é ChromaDB –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            total_size = self._get_directory_size(self.chromadb_dir)
            databases.append(DatabaseInfo(
                name="chromadb:main",
                type="chromadb",
                path=self.chromadb_dir,
                size_bytes=total_size,
                collections=[c.name for c in collections] if collections else []
            ))
            
        except ImportError:
            logger.warning("ChromaDB –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è ChromaDB: {e}", error=e)
        
        return databases
    
    def _discover_sqlite(self) -> List[DatabaseInfo]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
        databases: List[DatabaseInfo] = []
        
        # –ò—â–µ–º –≤—Å–µ .db –∏ .sqlite —Ñ–∞–π–ª—ã
        for pattern in ["**/*.db", "**/*.sqlite", "**/*.sqlite3"]:
            for db_file in self.base_dir.glob(pattern):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                if "test" in str(db_file) or "tmp" in str(db_file):
                    continue
                
                try:
                    size = db_file.stat().st_size
                    last_modified = datetime.fromtimestamp(db_file.stat().st_mtime)
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
                    record_count = None
                    try:
                        conn = sqlite3.connect(str(db_file))
                        cursor = conn.cursor()
                        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                        tables = cursor.fetchall()
                        if tables:
                            # –°—á–∏—Ç–∞–µ–º –∑–∞–ø–∏—Å–∏ –≤ –ø–µ—Ä–≤–æ–π —Ç–∞–±–ª–∏—Ü–µ
                            cursor.execute(f"SELECT COUNT(*) FROM {tables[0][0]}")
                            record_count = cursor.fetchone()[0]
                        conn.close()
                    except Exception:
                        pass
                    
                    databases.append(DatabaseInfo(
                        name=f"sqlite:{db_file.stem}",
                        type="sqlite",
                        path=db_file,
                        size_bytes=size,
                        last_modified=last_modified,
                        record_count=record_count
                    ))
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å SQLite —Ñ–∞–π–ª {db_file}: {e}")
        
        return databases
    
    def _discover_json_conversations(self) -> List[DatabaseInfo]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç JSON —Ñ–∞–π–ª—ã –¥–∏–∞–ª–æ–≥–æ–≤."""
        databases: List[DatabaseInfo] = []
        
        if not self.conversations_dir.exists():
            return databases
        
        total_size = 0
        file_count = 0
        
        for json_file in self.conversations_dir.glob("*.json"):
            try:
                size = json_file.stat().st_size
                total_size += size
                file_count += 1
            except Exception:
                continue
        
        if file_count > 0:
            databases.append(DatabaseInfo(
                name="json:conversations",
                type="json",
                path=self.conversations_dir,
                size_bytes=total_size,
                record_count=file_count
            ))
        
        return databases
    
    def _discover_caches(self) -> List[DatabaseInfo]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∫—ç—à–∏."""
        databases: List[DatabaseInfo] = []
        
        # Context cache
        if self.context_cache_dir.exists():
            size = self._get_directory_size(self.context_cache_dir)
            if size > 0:
                databases.append(DatabaseInfo(
                    name="cache:context",
                    type="cache",
                    path=self.context_cache_dir,
                    size_bytes=size
                ))
        
        return databases
    
    def _get_directory_size(self, path: Path) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –±–∞–π—Ç–∞—Ö."""
        total = 0
        try:
            for entry in path.rglob("*"):
                if entry.is_file():
                    total += entry.stat().st_size
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–¥—Å—á—ë—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ {path}: {e}")
        return total
    
    def backup_database(self, db_name: str, backup_name: Optional[str] = None) -> Path:
        """–°–æ–∑–¥–∞—ë—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –ë–î.
        
        Args:
            db_name: –ò–º—è –ë–î (–Ω–∞–ø—Ä–∏–º–µ—Ä, "chromadb:task_memory")
            backup_name: –ò–º—è –±—ç–∫–∞–ø–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è)
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –±—ç–∫–∞–ø—É
        """
        databases = self.discover_databases()
        db_info = next((db for db in databases if db.name == db_name), None)
        
        if not db_info:
            raise ValueError(f"–ë–î {db_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        if not backup_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{db_name.replace(':', '_')}_{timestamp}"
        
        backup_path = self.backup_dir / backup_name
        
        logger.info(f"üì¶ –°–æ–∑–¥–∞—é –±—ç–∫–∞–ø {db_name} -> {backup_path}")
        
        if db_info.type == "chromadb":
            # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é ChromaDB
            if backup_path.exists():
                shutil.rmtree(backup_path)
            shutil.copytree(db_info.path, backup_path)
        elif db_info.type == "sqlite":
            # –ö–æ–ø–∏—Ä—É–µ–º SQLite —Ñ–∞–π–ª
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(db_info.path, backup_path)
        elif db_info.type == "json":
            # –ö–æ–ø–∏—Ä—É–µ–º JSON –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            if backup_path.exists():
                shutil.rmtree(backup_path)
            shutil.copytree(db_info.path, backup_path)
        elif db_info.type == "cache":
            # –ö–æ–ø–∏—Ä—É–µ–º –∫—ç—à
            if backup_path.exists():
                shutil.rmtree(backup_path)
            shutil.copytree(db_info.path, backup_path)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –ë–î: {db_info.type}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±—ç–∫–∞–ø–∞
        metadata = {
            "db_name": db_name,
            "db_type": db_info.type,
            "original_path": str(db_info.path),
            "backup_path": str(backup_path),
            "created_at": datetime.now().isoformat(),
            "size_bytes": db_info.size_bytes,
            "record_count": db_info.record_count
        }
        
        metadata_path = backup_path.parent / f"{backup_name}.metadata.json"
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ –ë—ç–∫–∞–ø —Å–æ–∑–¥–∞–Ω: {backup_path} ({self._format_size(db_info.size_bytes)})")
        return backup_path
    
    def restore_database(self, backup_path: Path, target_db_name: Optional[str] = None) -> None:
        """–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ë–î –∏–∑ –±—ç–∫–∞–ø–∞.
        
        Args:
            backup_path: –ü—É—Ç—å –∫ –±—ç–∫–∞–ø—É
            target_db_name: –ò–º—è —Ü–µ–ª–µ–≤–æ–π –ë–î (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_path = backup_path.parent / f"{backup_path.name}.metadata.json"
        if metadata_path.exists():
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
            db_name = target_db_name or metadata["db_name"]
            db_type = metadata["db_type"]
            original_path = Path(metadata["original_path"])
        else:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            raise ValueError("–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±—ç–∫–∞–ø–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        logger.info(f"üîÑ –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é {db_name} –∏–∑ {backup_path}")
        
        # –°–æ–∑–¥–∞—ë–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é —Ç–µ–∫—É—â–µ–π –ë–î –ø–µ—Ä–µ–¥ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º
        if original_path.exists():
            logger.warning(f"‚ö†Ô∏è –¢–µ–∫—É—â–∞—è –ë–î —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞—é –±—ç–∫–∞–ø –ø–µ—Ä–µ–¥ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º")
            self.backup_database(db_name, f"{db_name}_before_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
        if db_type == "chromadb" or db_type == "json" or db_type == "cache":
            if original_path.exists():
                shutil.rmtree(original_path)
            original_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copytree(backup_path, original_path)
        elif db_type == "sqlite":
            original_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(backup_path, original_path)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –ë–î: {db_type}")
        
        logger.info(f"‚úÖ –ë–î {db_name} –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏–∑ {backup_path}")
    
    def cleanup_old_data(
        self,
        db_name: str,
        days: int = 30,
        dry_run: bool = True
    ) -> Dict[str, Any]:
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î.
        
        Args:
            db_name: –ò–º—è –ë–î
            days: –£–¥–∞–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π
            dry_run: –¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ, –Ω–µ —É–¥–∞–ª—è—Ç—å
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏
        """
        databases = self.discover_databases()
        db_info = next((db for db in databases if db.name == db_name), None)
        
        if not db_info:
            raise ValueError(f"–ë–î {db_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        cutoff_date = datetime.now() - timedelta(days=days)
        stats = {
            "db_name": db_name,
            "cutoff_date": cutoff_date.isoformat(),
            "dry_run": dry_run,
            "deleted_count": 0,
            "freed_bytes": 0
        }
        
        logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ {db_name} (–¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ä—à–µ {days} –¥–Ω–µ–π, dry_run={dry_run})")
        
        if db_info.type == "chromadb":
            stats.update(self._cleanup_chromadb(db_info, cutoff_date, dry_run))
        elif db_info.type == "json":
            stats.update(self._cleanup_json_conversations(db_info, cutoff_date, dry_run))
        elif db_info.type == "sqlite":
            stats.update(self._cleanup_sqlite(db_info, cutoff_date, dry_run))
        else:
            logger.warning(f"–û—á–∏—Å—Ç–∫–∞ –¥–ª—è —Ç–∏–ø–∞ {db_info.type} –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞")
        
        if not dry_run:
            logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É–¥–∞–ª–µ–Ω–æ {stats['deleted_count']} –∑–∞–ø–∏—Å–µ–π, –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ {self._format_size(stats['freed_bytes'])}")
        else:
            logger.info(f"‚ÑπÔ∏è Dry run: –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ {stats['deleted_count']} –∑–∞–ø–∏—Å–µ–π, –æ—Å–≤–æ–±–æ–¥–∏—Ç—Å—è {self._format_size(stats['freed_bytes'])}")
        
        return stats
    
    def _cleanup_chromadb(self, db_info: DatabaseInfo, cutoff_date: datetime, dry_run: bool) -> Dict[str, Any]:
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ ChromaDB."""
        # ChromaDB –Ω–µ —Ö—Ä–∞–Ω–∏—Ç timestamp –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –ø–æ—ç—Ç–æ–º—É –æ—á–∏—Å—Ç–∫–∞ —Å–ª–æ–∂–Ω–µ–µ
        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        return {"deleted_count": 0, "freed_bytes": 0}
    
    def _cleanup_json_conversations(self, db_info: DatabaseInfo, cutoff_date: datetime, dry_run: bool) -> Dict[str, Any]:
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ JSON –¥–∏–∞–ª–æ–≥–∏."""
        deleted_count = 0
        freed_bytes = 0
        
        for json_file in db_info.path.glob("*.json"):
            try:
                mtime = datetime.fromtimestamp(json_file.stat().st_mtime)
                if mtime < cutoff_date:
                    size = json_file.stat().st_size
                    if not dry_run:
                        json_file.unlink()
                    deleted_count += 1
                    freed_bytes += size
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {json_file}: {e}")
        
        return {"deleted_count": deleted_count, "freed_bytes": freed_bytes}
    
    def _cleanup_sqlite(self, db_info: DatabaseInfo, cutoff_date: datetime, dry_run: bool) -> Dict[str, Any]:
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ SQLite."""
        # –ü–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ - –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü
        return {"deleted_count": 0, "freed_bytes": 0}
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –ë–î.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        databases = self.discover_databases()
        
        total_size = sum(db.size_bytes for db in databases)
        total_records = sum(db.record_count or 0 for db in databases if db.record_count)
        
        by_type: Dict[str, List[DatabaseInfo]] = {}
        for db in databases:
            by_type.setdefault(db.type, []).append(db)
        
        return {
            "total_databases": len(databases),
            "total_size_bytes": total_size,
            "total_size_formatted": self._format_size(total_size),
            "total_records": total_records,
            "by_type": {
                db_type: {
                    "count": len(dbs),
                    "total_size": sum(d.size_bytes for d in dbs),
                    "total_size_formatted": self._format_size(sum(d.size_bytes for d in dbs))
                }
                for db_type, dbs in by_type.items()
            },
            "databases": [
                {
                    "name": db.name,
                    "type": db.type,
                    "path": str(db.path),
                    "size_bytes": db.size_bytes,
                    "size_formatted": self._format_size(db.size_bytes),
                    "record_count": db.record_count,
                    "collections": db.collections
                }
                for db in databases
            ]
        }
    
    def _format_size(self, bytes: int) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥."""
        for unit in ["B", "KB", "MB", "GB", "TB"]:
            if bytes < 1024.0:
                return f"{bytes:.2f} {unit}"
            bytes /= 1024.0
        return f"{bytes:.2f} PB"
