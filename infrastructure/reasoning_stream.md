# Reasoning Stream Module

## ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ

ĞœĞ¾Ğ´ÑƒĞ»ÑŒ `reasoning_stream.py` Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ **real-time ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³** Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ reasoning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (DeepSeek-R1, QwQ) Ğ² UI.

## Ğ—Ğ°Ñ‡ĞµĞ¼ ÑÑ‚Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾

Reasoning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² `<think>` Ğ±Ğ»Ğ¾ĞºĞ°Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ…:

1. **ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸** â€” Ğ¾Ğ½ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ñ…Ğ¾Ğ´ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
2. **ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµĞ¼ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ñ‚ÑŒ** â€” ĞµÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚ÑƒĞ´Ğ°, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ
3. **ĞŸĞ¾Ğ²Ñ‹ÑˆĞ°ĞµĞ¼ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ** â€” Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹
4. **Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ° ĞºĞ¾Ğ´Ğ°** â€” Ñ‚Ğ° Ğ¶Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ real-time Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°

## SSE Ğ¡Ğ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ

| Ğ¡Ğ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ | ĞšĞ¾Ğ³Ğ´Ğ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ | Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ |
|---------|-------------------|--------|
| `thinking_started` | ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ `<think>` Ğ±Ğ»Ğ¾ĞºĞ° | stage, total_chars |
| `thinking_in_progress` | ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‡Ğ°Ğ½Ğº | content, elapsed_ms |
| `thinking_completed` | `</think>` Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½ | content, summary |
| `thinking_interrupted` | ĞŸÑ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ | content |

## Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

### Real-time ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)

```python
from infrastructure.reasoning_stream import get_reasoning_stream_manager
from infrastructure.local_llm import create_llm_for_stage

llm = create_llm_for_stage("coding", model="deepseek-r1:7b")
manager = get_reasoning_stream_manager()

# Real-time ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ thinking Ğ¸ content
async for event_type, data in manager.stream_from_llm(llm, prompt, "coding"):
    if event_type == "thinking":
        yield data  # SSE ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ thinking Ğ±Ğ»Ğ¾ĞºĞ°
    elif event_type == "content":
        yield create_code_chunk_event(data)  # Ğ§Ğ°Ğ½Ğº ĞºĞ¾Ğ´Ğ°
    elif event_type == "done":
        final_code = extract_code_from_reasoning(data)
```

### Post-hoc Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° (Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸)

```python
from infrastructure.reasoning_stream import get_reasoning_stream_manager

manager = get_reasoning_stream_manager()

# ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¾Ñ‚ LLM
async for event in manager.process_response_with_thinking(
    response=llm_response,
    stage="coding"
):
    yield event  # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² SSE stream
```

### Ğ’ workflow node

```python
from infrastructure.reasoning_stream import get_reasoning_stream_manager

@workflow_node(stage="coding")
async def coder_node(state: AgentState) -> AgentState:
    manager = get_reasoning_stream_manager()
    
    response = await asyncio.to_thread(llm.generate, prompt)
    
    # Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ¼ thinking ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
    async for event in manager.process_response_with_thinking(
        response=response,
        stage="coding"
    ):
        # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· SSE (ĞµÑĞ»Ğ¸ enable_sse=True)
        if state.get("enable_sse"):
            yield event
    
    # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ state
    parsed = parse_reasoning_response(response)
    state["code"] = parsed.answer
    
    return state
```

### ĞŸÑ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ

```python
# Ğ’ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞµ "Ğ¡Ñ‚Ğ¾Ğ¿" ĞºĞ½Ğ¾Ğ¿ĞºĞ¸
manager = get_reasoning_stream_manager()
manager.interrupt()
```

## ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ

### config.toml

```toml
[reasoning]
show_thinking = true        # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ <think> Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ² UI

[streaming]
enabled = true              # Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ real-time ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³
thinking_chunk_size = 100   # Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ñ‡Ğ°Ğ½ĞºĞ° thinking (ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²)
thinking_debounce_ms = 50   # Ğ—Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡Ğ°Ğ½ĞºĞ°Ğ¼Ğ¸
max_thinking_time_ms = 120000  # ĞœĞ°ĞºÑ. Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ
use_streaming_agents = true # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ StreamingCoderAgent
```

### ĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾

```python
from infrastructure.reasoning_stream import (
    ReasoningStreamManager,
    ReasoningStreamConfig
)

config = ReasoningStreamConfig(
    enabled=True,
    chunk_size=200,
    debounce_ms=100
)

manager = ReasoningStreamManager(config)
```

## Frontend Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ

### ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹

```typescript
eventSource.addEventListener('thinking_started', (e) => {
  const data = JSON.parse(e.data);
  showThinkingIndicator(data.stage, data.total_chars);
});

eventSource.addEventListener('thinking_in_progress', (e) => {
  const data = JSON.parse(e.data);
  appendThinkingContent(data.content);
  updateProgress(data.elapsed_ms);
});

eventSource.addEventListener('thinking_completed', (e) => {
  const data = JSON.parse(e.data);
  showThinkingSummary(data.summary);
  hideThinkingIndicator();
});

eventSource.addEventListener('thinking_interrupted', (e) => {
  showInterruptedMessage();
});
```

### UI ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€)

```tsx
function ThinkingBlock({ stage, content, status, summary }) {
  const [expanded, setExpanded] = useState(false);
  
  if (status === 'completed') {
    return (
      <div className="thinking-block">
        <button onClick={() => setExpanded(!expanded)}>
          ğŸ’­ {expanded ? 'Ğ¡ĞºÑ€Ñ‹Ñ‚ÑŒ' : 'ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ'} Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ
        </button>
        {!expanded && <p className="summary">{summary}</p>}
        {expanded && <pre className="thinking-content">{content}</pre>}
      </div>
    );
  }
  
  return (
    <div className="thinking-block in-progress">
      <span className="spinner" />
      <span>Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ...</span>
      <pre className="thinking-content">{content}</pre>
    </div>
  );
}
```

## ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

### Real-time ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ (Ğ½Ğ¾Ğ²Ñ‹Ğ¹)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LocalLLM          â”‚
â”‚   generate_stream() â”‚ â—„â”€â”€ Ollama streaming API
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ StreamChunk (is_thinking, content)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReasoningStreamManager  â”‚
â”‚ stream_from_llm()       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ ("thinking", event) / ("content", chunk) / ("done", full)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   agent.py      â”‚ â—„â”€â”€ Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ SSE ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ
â”‚   SSEManager    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ SSE events
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚
â”‚   EventSource   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Post-hoc Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° (legacy)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LocalLLM      â”‚
â”‚   generate()    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ response Ñ <think>
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReasoningStreamManager  â”‚
â”‚ process_response_with_  â”‚
â”‚ thinking()              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ SSE events (ÑĞ¼ÑƒĞ»ÑÑ†Ğ¸Ñ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ°)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹

- `infrastructure/reasoning_utils.py` â€” Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ `<think>` Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²
- `backend/sse_manager.py` â€” Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ SSE ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ
- `infrastructure/local_llm.py` â€” Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²
- `.cursor/rules/models.md` â€” Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼

## Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

- [x] Backend Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ `reasoning_stream.py`
- [x] SSE ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ: `thinking_started`, `thinking_in_progress`, `thinking_completed`, `thinking_interrupted`
- [x] Frontend ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ Ğ² `constants/sse.ts`
- [x] Hook Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ² `useAgentStream.ts`
- [x] UI ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ `ThinkingBlock.tsx` (ÑĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹, ĞºĞ°Ğº Ğ² ChatGPT)
- [x] Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² `ProgressSteps.tsx` (Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ°)
- [x] Real-time ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ñ‡ĞµÑ€ĞµĞ· `LocalLLM.generate_stream()`
- [x] `stream_from_llm()` â€” real-time Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ thinking/content
- [x] Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹: `StreamingPlannerAgent`, `StreamingCoderAgent` Ğ¸ Ğ´Ñ€.
- [x] Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ ÑƒĞ·Ğ»Ñ‹ Ğ² `workflow_nodes.py`: `stream_planner_node()`, `stream_coder_node()` Ğ¸ Ğ´Ñ€.
- [x] Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² `agent.py`: `run_workflow_stream_with_thinking()`
- [x] ĞĞ²Ñ‚Ğ¾Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· `_is_streaming_enabled()` Ğ¸Ğ· config.toml
- [ ] Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ ĞºĞ¾Ğ´Ğ° Ğ² IDE panel

## ĞšĞ°Ğº Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³

### config.toml

```toml
[streaming]
use_streaming_agents = true   # Ğ’ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ thinking ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³
```

### Ğ§Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚

1. `_is_streaming_enabled()` Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ„Ğ»Ğ°Ğ³
2. Ğ•ÑĞ»Ğ¸ `true` â†’ `run_workflow_stream_with_thinking()` 
3. Ğ•ÑĞ»Ğ¸ `false` â†’ `run_workflow_stream()` (ÑÑ‚Ğ°Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ)

### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° (Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾)

```
backend/routers/agent.py
    â”‚
    â”œâ”€â”€ _is_streaming_enabled() Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ config.toml
    â”‚
    â”œâ”€â”€ run_workflow_stream_with_thinking()
    â”‚   â”œâ”€â”€ stream_planner_node() â†’ thinking SSE
    â”‚   â”œâ”€â”€ stream_generator_node() â†’ thinking SSE
    â”‚   â”œâ”€â”€ stream_coder_node() â†’ thinking + code_chunk SSE
    â”‚   â”œâ”€â”€ stream_debugger_node() â†’ thinking SSE
    â”‚   â”œâ”€â”€ stream_fixer_node() â†’ thinking + code_chunk SSE
    â”‚   â”œâ”€â”€ stream_reflection_node() â†’ thinking SSE
    â”‚   â””â”€â”€ stream_critic_node() â†’ thinking SSE
    â”‚
    â””â”€â”€ run_workflow_stream() (legacy, LangGraph)
```

## TODO

- [ ] ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ `backend/routers/agent.py` Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ stream_*_node()
- [ ] Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ "Ğ¡Ñ‚Ğ¾Ğ¿" Ğ´Ğ»Ñ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹
- [ ] Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ thinking Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°
- [ ] Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² performance_metrics
- [ ] Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

## ĞŸĞ»Ğ°Ğ½ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ€Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°

Ğ¡Ğ¼. `DEPRECATION.md` Ğ² ĞºĞ¾Ñ€Ğ½Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°.
