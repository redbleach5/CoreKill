"""–°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –∞–≥–µ–Ω—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞.

–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç real-time —Å—Ç—Ä–∏–º–∏–Ω–≥:
- <think> –±–ª–æ–∫–æ–≤ reasoning –º–æ–¥–µ–ª–µ–π
- –ö–æ–¥–∞ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LocalLLM.generate_stream() –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞.
–°–æ–≤–º–µ—Å—Ç–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º CoderAgent —á–µ—Ä–µ–∑ –æ–±—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –∏ –æ—á–∏—Å—Ç–∫—É –∫–æ–¥–∞.

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    ```python
    from agents.streaming_coder import StreamingCoderAgent
    
    agent = StreamingCoderAgent(model="deepseek-r1:7b")
    
    async for event_type, data in agent.generate_code_stream(plan, tests, context, intent):
        if event_type == "thinking":
            yield data  # SSE —Å–æ–±—ã—Ç–∏–µ –¥–ª—è UI
        elif event_type == "code_chunk":
            yield code_chunk_event(data)
        elif event_type == "done":
            final_code = data
    ```
"""
from typing import Optional, Dict, Any, AsyncGenerator, TYPE_CHECKING
from infrastructure.local_llm import create_llm_for_stage, StreamChunk
from infrastructure.prompt_enhancer import get_prompt_enhancer
from infrastructure.reasoning_stream import get_reasoning_stream_manager
from infrastructure.coder_prompt_builder import get_coder_prompt_builder
from utils.logger import get_logger
from utils.config import get_config
from utils.intent_helpers import get_intent_description
from agents.base import BaseAgent

if TYPE_CHECKING:
    from infrastructure.coder_prompt_builder import CoderPromptBuilder

logger = get_logger()


class StreamingCoderAgent(BaseAgent):
    """–ê–≥–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å real-time —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º.
    
    –†–∞—Å—à–∏—Ä—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å CoderAgent:
    - Real-time —Å—Ç—Ä–∏–º–∏–Ω–≥ <think> –±–ª–æ–∫–æ–≤
    - Real-time —Å—Ç—Ä–∏–º–∏–Ω–≥ –∫–æ–¥–∞
    - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    
    –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π generate_code().
    """

    def __init__(
        self, 
        model: Optional[str] = None, 
        temperature: float = 0.25,
        user_query: str = ""
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞.
        
        Args:
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.15-0.35)
            user_query: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ (LLM —Å–æ–∑–¥–∞—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        super().__init__(
            model=model,
            temperature=temperature,
            stage="coding"
        )
        
        self.user_query = user_query
        self.prompt_enhancer = get_prompt_enhancer()
        self.prompt_builder = get_coder_prompt_builder()
        self.reasoning_manager = get_reasoning_stream_manager()
        self._interrupted = False
    
    def interrupt(self) -> None:
        """–ü—Ä–µ—Ä—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é."""
        self._interrupted = True
        self.reasoning_manager.interrupt()
        logger.info("‚èπÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞")
    
    async def generate_code_stream(
        self,
        plan: str,
        tests: str,
        context: str,
        intent_type: str,
        user_query: str = "",
        stage: str = "coding"
    ) -> AsyncGenerator[tuple[str, str], None]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–¥ —Å real-time —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º.
        
        Args:
            plan: –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
            tests: –¢–µ—Å—Ç—ã pytest
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG
            intent_type: –¢–∏–ø –Ω–∞–º–µ—Ä–µ–Ω–∏—è (create/modify/debug/etc)
            user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            stage: –≠—Ç–∞–ø workflow (–¥–ª—è SSE —Å–æ–±—ã—Ç–∏–π)
            
        Yields:
            tuple[event_type, data]:
                - ("thinking", sse_event) ‚Äî SSE —Å–æ–±—ã—Ç–∏–µ –¥–ª—è <think> –±–ª–æ–∫–∞
                - ("code_chunk", chunk) ‚Äî —á–∞–Ω–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞
                - ("done", final_code) ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π –∫–æ–¥
                
        Example:
            async for event_type, data in agent.generate_code_stream(...):
                if event_type == "thinking":
                    yield data  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º SSE
                elif event_type == "code_chunk":
                    yield create_code_chunk_sse(data)
                elif event_type == "done":
                    save_code(data)
        """
        logger.info(f"üíª –°—Ç—Ä–∏–º–∏–Ω–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –¥–ª—è: {intent_type}")
        
        self.reset()
        
        # –ù–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–¥ –¥–ª—è –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π
        if intent_type == "greeting":
            logger.info("‚ÑπÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è")
            yield ("done", "")
            return
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º thinking –æ –Ω–∞—á–∞–ª–µ –∞–Ω–∞–ª–∏–∑–∞
        from datetime import datetime
        from infrastructure.reasoning_stream import ThinkingChunk, ThinkingStatus
        start_time = datetime.now()
        
        yield ("thinking", await self.reasoning_manager.create_thinking_event(
            ThinkingChunk(
                content="–ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞...",
                status=ThinkingStatus.IN_PROGRESS,
                stage=stage,
                elapsed_ms=0,
                total_chars=0
            )
        ))
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º thinking
        if context:
            context_preview = context[:200] + "..." if len(context) > 200 else context
            yield ("thinking", await self.reasoning_manager.create_thinking_event(
                ThinkingChunk(
                    content=f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ ({len(context)} —Å–∏–º–≤–æ–ª–æ–≤): {context_preview}",
                    status=ThinkingStatus.IN_PROGRESS,
                    stage=stage,
                    elapsed_ms=int((datetime.now() - start_time).total_seconds() * 1000),
                    total_chars=0
                )
            ))
        
        # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
        query = user_query or self.user_query
        if query:
            prompt = self.prompt_enhancer.enhance_for_coding(
                user_query=query,
                intent_type=intent_type,
                plan=plan,
                tests=tests,
                context=context
            )
        else:
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º prompt_builder –≤–º–µ—Å—Ç–æ —É–¥–∞–ª–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
            prompt = self.prompt_builder.build_generation_prompt(
                plan=plan,
                tests=tests,
                context=context,
                intent_type=intent_type,
                user_query=query
            )
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º thinking –æ –Ω–∞—á–∞–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        yield ("thinking", await self.reasoning_manager.create_thinking_event(
            ThinkingChunk(
                content=f"–ù–∞—á–∏–Ω–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ —Ç–∏–ø–∞ '{intent_type}'. –ü–ª–∞–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç {len(plan.split(chr(10)))} —à–∞–≥–æ–≤.",
                status=ThinkingStatus.IN_PROGRESS,
                stage=stage,
                elapsed_ms=int((datetime.now() - start_time).total_seconds() * 1000),
                total_chars=0
            )
        ))
        
        config = get_config()
        code_buffer = ""
        full_response = ""
        
        try:
            # Real-time —Å—Ç—Ä–∏–º–∏–Ω–≥ —á–µ—Ä–µ–∑ reasoning_manager
            async for event_type, data in self.reasoning_manager.stream_from_llm(
                llm=self.llm,
                prompt=prompt,
                stage=stage,
                num_predict=config.llm_tokens_code
            ):
                if self._interrupted:
                    logger.info("‚èπÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                    break
                
                if event_type == "thinking":
                    # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º SSE —Å–æ–±—ã—Ç–∏–µ –¥–ª—è thinking
                    yield ("thinking", data)
                    
                elif event_type == "progress":
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º progress —Å–æ–±—ã—Ç–∏—è –¥–ª—è non-reasoning –º–æ–¥–µ–ª–µ–π
                    yield ("progress", data)
                    
                elif event_type == "content":
                    # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –∫–æ–¥ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫
                    code_buffer += data
                    yield ("code_chunk", data)
                    
                elif event_type == "done":
                    full_response = data
            
            # –û—á–∏—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –∏–∑ BaseAgent)
            if full_response:
                cleaned_code = self._clean_code_from_reasoning(full_response)
            else:
                cleaned_code = self._clean_code(code_buffer)
            
            if cleaned_code:
                logger.info(f"‚úÖ –ö–æ–¥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ({len(cleaned_code)} —Å–∏–º–≤–æ–ª–æ–≤)")
            else:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–¥")
            
            yield ("done", cleaned_code)
            
        except Exception as e:
            from infrastructure.local_llm import LLMModelUnavailableError
            
            if isinstance(e, LLMModelUnavailableError):
                logger.warning(
                    f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {e.model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞: {e}. "
                    f"–ü—Ä–æ–±—É—é –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å..."
                )
                
                # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
                if self._switch_to_fallback_model(
                    failed_model=e.model,
                    task_type="coding",
                    complexity=getattr(e, 'complexity', None)
                ):
                    logger.info(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –º–æ–¥–µ–ª—å {self.model}, –ø–æ–≤—Ç–æ—Ä—è—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
                    
                    # –í–ê–ñ–ù–û: –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –ø—Ä–æ–º–ø—Ç –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
                    # (–º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, –ø—Ä–æ–º–ø—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º)
                    query = user_query or self.user_query
                    if query:
                        prompt = self.prompt_enhancer.enhance_for_coding(
                            user_query=query,
                            intent_type=intent_type,
                            plan=plan,
                            tests=tests,
                            context=context
                        )
                    else:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º prompt_builder –≤–º–µ—Å—Ç–æ —É–¥–∞–ª–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
                        prompt = self.prompt_builder.build_generation_prompt(
                            plan=plan,
                            tests=tests,
                            context=context,
                            intent_type=intent_type,
                            user_query=query
                        )
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º thinking –æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏
                    yield ("thinking", await self.reasoning_manager.create_thinking_event(
                        ThinkingChunk(
                            content=f"–ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –º–æ–¥–µ–ª—å {self.model}. –ü—Ä–æ–¥–æ–ª–∂–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞...",
                            status=ThinkingStatus.IN_PROGRESS,
                            stage=stage,
                            elapsed_ms=int((datetime.now() - start_time).total_seconds() * 1000),
                            total_chars=0
                        )
                    ))
                    
                    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                    try:
                        code_buffer = ""
                        full_response = ""
                        
                        async for event_type, data in self.reasoning_manager.stream_from_llm(
                            llm=self.llm,
                            prompt=prompt,
                            stage=stage,
                            num_predict=config.llm_tokens_code
                        ):
                            if self._interrupted:
                                logger.info("‚èπÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                                break
                            
                            if event_type == "thinking":
                                yield ("thinking", data)
                            elif event_type == "content":
                                code_buffer += data
                                yield ("code_chunk", data)
                            elif event_type == "done":
                                full_response = data
                        
                        # –û—á–∏—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥
                        if full_response:
                            cleaned_code = self._clean_code_from_reasoning(full_response)
                        else:
                            cleaned_code = self._clean_code(code_buffer)
                        
                        if cleaned_code:
                            logger.info(f"‚úÖ –ö–æ–¥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é ({len(cleaned_code)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        else:
                            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–¥ –¥–∞–∂–µ —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é")
                        
                        yield ("done", cleaned_code)
                        return
                        
                    except Exception as retry_error:
                        logger.error(
                            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–µ —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é {self.model}: {retry_error}",
                            error=retry_error
                        )
                        yield ("done", "")
                else:
                    logger.error(
                        f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å. "
                        f"–ö–æ–¥ –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω."
                    )
                    yield ("done", "")
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –∫–æ–¥–∞: {e}", error=e)
                yield ("done", "")
    
    async def fix_code_stream(
        self,
        code: str,
        instructions: str,
        tests: str,
        validation_results: Dict[str, Any],
        stage: str = "fixing"
    ) -> AsyncGenerator[tuple[str, str], None]:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–¥ —Å real-time —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º.
        
        Args:
            code: –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å –æ—à–∏–±–∫–∞–º–∏
            instructions: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ—Ç Debugger
            tests: –¢–µ—Å—Ç—ã
            validation_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            stage: –≠—Ç–∞–ø workflow
            
        Yields:
            tuple[event_type, data] ‚Äî –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ generate_code_stream
        """
        logger.info("üîß –°—Ç—Ä–∏–º–∏–Ω–≥ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–¥–∞...")
        
        self.reset()
        
        if not code.strip() or not instructions.strip():
            logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∫–æ–¥ –∏–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏")
            yield ("done", code)
            return
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º prompt_builder –≤–º–µ—Å—Ç–æ —É–¥–∞–ª–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
        prompt = self.prompt_builder.build_fix_prompt(
            code=code,
            instructions=instructions,
            tests=tests,
            validation_results=validation_results
        )
        
        config = get_config()
        code_buffer = ""
        full_response = ""
        
        try:
            async for event_type, data in self.reasoning_manager.stream_from_llm(
                llm=self.llm,
                prompt=prompt,
                stage=stage,
                num_predict=config.llm_tokens_code
            ):
                if self._interrupted:
                    break
                
                if event_type == "thinking":
                    yield ("thinking", data)
                elif event_type == "content":
                    code_buffer += data
                    yield ("code_chunk", data)
                elif event_type == "done":
                    full_response = data
            
            # –û—á–∏—â–∞–µ–º –∫–æ–¥ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –∏–∑ BaseAgent)
            if full_response:
                cleaned_code = self._clean_code_from_reasoning(full_response)
            else:
                cleaned_code = self._clean_code(code_buffer)
            
            if not cleaned_code:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∫–æ–¥, –≤–æ–∑–≤—Ä–∞—â–∞—é –∏—Å—Ö–æ–¥–Ω—ã–π")
                cleaned_code = code
            else:
                logger.info(f"‚úÖ –ö–æ–¥ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω ({len(cleaned_code)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            yield ("done", cleaned_code)
            
        except Exception as e:
            from infrastructure.local_llm import LLMModelUnavailableError
            
            if isinstance(e, LLMModelUnavailableError):
                logger.warning(
                    f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {e.model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –ø—Ä–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–¥–∞: {e}. "
                    f"–ü—Ä–æ–±—É—é –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å..."
                )
                
                # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
                if self._switch_to_fallback_model(
                    failed_model=e.model,
                    task_type="fixing",
                    complexity=getattr(e, 'complexity', None)
                ):
                    logger.info(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –º–æ–¥–µ–ª—å {self.model}, –ø–æ–≤—Ç–æ—Ä—è—é –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ...")
                    
                    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                    try:
                        code_buffer = ""
                        full_response = ""
                        
                        async for event_type, data in self.reasoning_manager.stream_from_llm(
                            llm=self.llm,
                            prompt=prompt,
                            stage=stage,
                            num_predict=config.llm_tokens_code
                        ):
                            if self._interrupted:
                                break
                            
                            if event_type == "thinking":
                                yield ("thinking", data)
                            elif event_type == "content":
                                code_buffer += data
                                yield ("code_chunk", data)
                            elif event_type == "done":
                                full_response = data
                        
                        # –û—á–∏—â–∞–µ–º –∫–æ–¥
                        if full_response:
                            cleaned_code = self._clean_code_from_reasoning(full_response)
                        else:
                            cleaned_code = self._clean_code(code_buffer)
                        
                        if not cleaned_code:
                            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∫–æ–¥ –¥–∞–∂–µ —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é, –≤–æ–∑–≤—Ä–∞—â–∞—é –∏—Å—Ö–æ–¥–Ω—ã–π")
                            cleaned_code = code
                        else:
                            logger.info(f"‚úÖ –ö–æ–¥ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é ({len(cleaned_code)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        
                        yield ("done", cleaned_code)
                        return
                        
                    except Exception as retry_error:
                        logger.error(
                            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–µ —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é {self.model}: {retry_error}",
                            error=retry_error
                        )
                        yield ("done", code)
                else:
                    logger.error(
                        f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å. "
                        f"–í–æ–∑–≤—Ä–∞—â–∞—é –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥."
                    )
                    yield ("done", code)
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {e}", error=e)
                yield ("done", code)
    
    # === –ü—Ä–∏–≤–∞—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã (–æ–±—â–∏–µ —Å CoderAgent) ===
    
    # _build_code_generation_prompt –∏ _build_fix_prompt —É–¥–∞–ª–µ–Ω—ã –≤ –ø–æ–ª—å–∑—É CoderPromptBuilder


    # === Factory —Ñ—É–Ω–∫—Ü–∏—è ===

def get_streaming_coder_agent(
    model: Optional[str] = None,
    temperature: float = 0.25
) -> StreamingCoderAgent:
    """–°–æ–∑–¥–∞—ë—Ç StreamingCoderAgent —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞.
    
    Args:
        model: –ú–æ–¥–µ–ª—å (–µ—Å–ª–∏ None, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π StreamingCoderAgent
    """
    return StreamingCoderAgent(model=model, temperature=temperature)
