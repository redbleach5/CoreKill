"""–°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –∞–≥–µ–Ω—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤.

–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç real-time —Å—Ç—Ä–∏–º–∏–Ω–≥:
- <think> –±–ª–æ–∫–æ–≤ reasoning –º–æ–¥–µ–ª–µ–π
- –¢–µ—Å—Ç–æ–≤ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è
"""
from typing import Optional, AsyncGenerator
from infrastructure.local_llm import create_llm_for_stage
from infrastructure.prompt_enhancer import get_prompt_enhancer
from infrastructure.reasoning_stream import get_reasoning_stream_manager
from infrastructure.reasoning_utils import extract_code_from_reasoning, is_reasoning_response
from agents.base import BaseAgent
from utils.logger import get_logger
from utils.config import get_config
from infrastructure.model_router import get_model_router

logger = get_logger()


class StreamingTestGeneratorAgent(BaseAgent):
    """–ê–≥–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ pytest —Ç–µ—Å—Ç–æ–≤ —Å real-time —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º.
    
    –†–∞—Å—à–∏—Ä—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å TestGeneratorAgent:
    - Real-time —Å—Ç—Ä–∏–º–∏–Ω–≥ <think> –±–ª–æ–∫–æ–≤
    - Real-time —Å—Ç—Ä–∏–º–∏–Ω–≥ —Ç–µ—Å—Ç–æ–≤
    - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    """

    def __init__(
        self, 
        model: Optional[str] = None, 
        temperature: float = 0.18
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞.
        
        Args:
            model: –ú–æ–¥–µ–ª—å (–µ—Å–ª–∏ None, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0.15-0.2 –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ (LLM —Å–æ–∑–¥–∞—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        super().__init__(
            model=model,
            temperature=temperature,
            stage="testing"
        )
        self.prompt_enhancer = get_prompt_enhancer()
        self.reasoning_manager = get_reasoning_stream_manager()
        self._interrupted = False
    
    def interrupt(self) -> None:
        """–ü—Ä–µ—Ä—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é."""
        self._interrupted = True
        self.reasoning_manager.interrupt()
        logger.info("‚èπÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤ –ø—Ä–µ—Ä–≤–∞–Ω–∞")
    
    async def generate_tests_stream(
        self,
        plan: str,
        context: str,
        intent_type: str,
        user_query: str = "",
        min_test_cases: int = 3,
        max_test_cases: int = 5,
        stage: str = "testing"
    ) -> AsyncGenerator[tuple[str, str], None]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç—ã —Å real-time —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º.
        
        Args:
            plan: –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG
            intent_type: –¢–∏–ø –Ω–∞–º–µ—Ä–µ–Ω–∏—è
            user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            min_test_cases: –ú–∏–Ω. –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤
            max_test_cases: –ú–∞–∫—Å. –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤
            stage: –≠—Ç–∞–ø workflow
            
        Yields:
            tuple[event_type, data]:
                - ("thinking", sse_event) ‚Äî SSE —Å–æ–±—ã—Ç–∏–µ –¥–ª—è <think> –±–ª–æ–∫–∞
                - ("test_chunk", chunk) ‚Äî —á–∞–Ω–∫ —Ç–µ—Å—Ç–æ–≤
                - ("done", final_tests) ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
        """
        logger.info(f"üß™ –°—Ç—Ä–∏–º–∏–Ω–≥ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è: {intent_type} (–ø–ª–∞–Ω: {len(plan)} —Å–∏–º–≤., –∫–æ–Ω—Ç–µ–∫—Å—Ç: {len(context)} —Å–∏–º–≤.)")
        
        self.reset()
        
        # –ù–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π
        if intent_type == "greeting":
            logger.info("‚ÑπÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è")
            yield ("done", "")
            return
        
        # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
        if user_query:
            prompt = self.prompt_enhancer.enhance_for_tests(
                user_query=user_query,
                intent_type=intent_type,
                context=context
            )
        else:
            prompt = self._build_test_generation_prompt(
                plan=plan,
                context=context,
                intent_type=intent_type,
                min_cases=min_test_cases,
                max_cases=max_test_cases
            )
        
        config = get_config()
        tests_buffer = ""
        full_response = ""
        
        try:
            async for event_type, data in self.reasoning_manager.stream_from_llm(
                llm=self.llm,
                prompt=prompt,
                stage=stage,
                num_predict=config.llm_tokens_tests
            ):
                if self._interrupted:
                    logger.info("‚èπÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞")
                    break
                
                if event_type == "thinking":
                    yield ("thinking", data)
                elif event_type == "progress":
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º progress —Å–æ–±—ã—Ç–∏—è –¥–ª—è non-reasoning –º–æ–¥–µ–ª–µ–π
                    yield ("progress", data)
                elif event_type == "content":
                    tests_buffer += data
                    yield ("test_chunk", data)
                elif event_type == "done":
                    full_response = data
            
            # –û—á–∏—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
            if full_response:
                if is_reasoning_response(full_response):
                    tests_only = extract_code_from_reasoning(full_response)
                    cleaned_tests = self._clean_test_code(tests_only)
                else:
                    cleaned_tests = self._clean_test_code(full_response)
            else:
                cleaned_tests = self._clean_test_code(tests_buffer)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ—Å—Ç—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã
            if cleaned_tests:
                logger.info(f"‚úÖ –¢–µ—Å—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã ({len(cleaned_tests)} —Å–∏–º–≤–æ–ª–æ–≤)")
            else:
                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                logger.error(
                    f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç—ã! "
                    f"full_response: {len(full_response) if full_response else 0} —Å–∏–º–≤–æ–ª–æ–≤, "
                    f"tests_buffer: {len(tests_buffer)} —Å–∏–º–≤–æ–ª–æ–≤, "
                    f"model: {self.model}, stage: {stage}"
                )
                # –ï—Å–ª–∏ –µ—Å—Ç—å full_response, –ª–æ–≥–∏—Ä—É–µ–º –µ–≥–æ –Ω–∞—á–∞–ª–æ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                if full_response and len(full_response) > 0:
                    preview = full_response[:200].replace('\n', '\\n')
                    logger.debug(f"üîç –ù–∞—á–∞–ª–æ full_response: {preview}...")
            
            yield ("done", cleaned_tests)
            
        except Exception as e:
            from infrastructure.local_llm import LLMModelUnavailableError
            
            if isinstance(e, LLMModelUnavailableError):
                logger.warning(
                    f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {e.model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤: {e}. "
                    f"–ü—Ä–æ–±—É—é –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å..."
                )
                
                # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
                if self._switch_to_fallback_model(
                    failed_model=e.model,
                    task_type="testing",
                    complexity=getattr(e, 'complexity', None)
                ):
                    logger.info(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ –º–æ–¥–µ–ª—å {self.model}, –ø–æ–≤—Ç–æ—Ä—è—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
                    
                    # –í–ê–ñ–ù–û: –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –ø—Ä–æ–º–ø—Ç –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
                    if user_query:
                        prompt = self.prompt_enhancer.enhance_for_tests(
                            user_query=user_query,
                            intent_type=intent_type,
                            context=context
                        )
                    else:
                        prompt = self._build_test_generation_prompt(
                            plan=plan,
                            context=context,
                            intent_type=intent_type,
                            min_cases=min_test_cases,
                            max_cases=max_test_cases
                        )
                    
                    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                    try:
                        tests_buffer = ""
                        full_response = ""
                        
                        async for event_type, data in self.reasoning_manager.stream_from_llm(
                            llm=self.llm,
                            prompt=prompt,
                            stage=stage,
                            num_predict=config.llm_tokens_tests
                        ):
                            if self._interrupted:
                                logger.info("‚èπÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞")
                                break
                            
                            if event_type == "thinking":
                                yield ("thinking", data)
                            elif event_type == "content":
                                tests_buffer += data
                                yield ("test_chunk", data)
                            elif event_type == "done":
                                full_response = data
                        
                        # –û—á–∏—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
                        if full_response:
                            if is_reasoning_response(full_response):
                                tests_only = extract_code_from_reasoning(full_response)
                                cleaned_tests = self._clean_test_code(tests_only)
                            else:
                                cleaned_tests = self._clean_test_code(full_response)
                        else:
                            cleaned_tests = self._clean_test_code(tests_buffer)
                        
                        if cleaned_tests:
                            logger.info(f"‚úÖ –¢–µ—Å—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é ({len(cleaned_tests)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        else:
                            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç—ã –¥–∞–∂–µ —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é")
                        
                        yield ("done", cleaned_tests)
                        return
                        
                    except Exception as retry_error:
                        logger.error(
                            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–µ —Å –∑–∞–ø–∞—Å–Ω–æ–π –º–æ–¥–µ–ª—å—é {self.model}: {retry_error}",
                            error=retry_error
                        )
                        yield ("done", "")
                else:
                    logger.error(
                        f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å. "
                        f"–¢–µ—Å—Ç—ã –Ω–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã."
                    )
                    yield ("done", "")
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —Ç–µ—Å—Ç–æ–≤: {e}", error=e)
                yield ("done", "")
    
    # === –ü—Ä–∏–≤–∞—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã ===
    
    def _build_test_generation_prompt(
        self,
        plan: str,
        context: str,
        intent_type: str,
        min_cases: int,
        max_cases: int
    ) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤."""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è intent
        from utils.intent_helpers import get_intent_description
        intent_desc = get_intent_description(intent_type, format="planning") or "–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏"
        
        context_section = ""
        if context.strip():
            context_section = f"""
–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:
{context}
"""
        
        prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–∞–ø–∏—Å–∞–Ω–∏—é pytest —Ç–µ—Å—Ç–æ–≤. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ç–µ—Å—Ç—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–∏.

–¢–∏–ø –∑–∞–¥–∞—á–∏: {intent_desc}

–ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:
{plan}
{context_section}
–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –ê–ù–ê–õ–ò–ó–ò–†–£–ô —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–ª–∞–µ—Ç:
   - –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç print() –¥–ª—è –≤—ã–≤–æ–¥–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π capsys –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ stdout
   - –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ (return) ‚Äî –ø—Ä–æ–≤–µ—Ä—è–π –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
   - –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–º–µ–Ω—è–µ—Ç –æ–±—ä–µ–∫—Ç ‚Äî –ø—Ä–æ–≤–µ—Ä—è–π –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞
   - –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ —Ñ–∞–π–ª ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π tmp_path –∏ –ø—Ä–æ–≤–µ—Ä—è–π —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ

2. –ü–†–ò–ú–ï–†–´ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:

   # –î–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ —Å print() ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π capsys:
   def test_hello_world(capsys):
       hello_world()  # —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–∑—ã–≤–∞–µ—Ç print()
       captured = capsys.readouterr()
       assert "Hello" in captured.out
   
   # –î–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ —Å return ‚Äî –ø—Ä–æ–≤–µ—Ä—è–π –∑–Ω–∞—á–µ–Ω–∏–µ:
   def test_add():
       result = add(2, 3)
       assert result == 5
   
   # –î–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑–º–µ–Ω—è—é—â–µ–π —Å–ø–∏—Å–æ–∫ ‚Äî –ø—Ä–æ–≤–µ—Ä—è–π –∏–∑–º–µ–Ω–µ–Ω–∏–µ:
   def test_append_item():
       items = []
       append_item(items, "x")
       assert "x" in items

3. –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
   - –ù–∞–ø–∏—à–∏ {min_cases}-{max_cases} –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤
   - –ü–æ–∫—Ä—ã–≤–∞–π: –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏, –æ—à–∏–±–∫–∏
   - –ü–æ–Ω—è—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ —Ç–µ—Å—Ç–æ–≤: test_—Ñ—É–Ω–∫—Ü–∏—è_—Å—Ü–µ–Ω–∞—Ä–∏–π
   - –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π parametrize –µ—Å–ª–∏ —ç—Ç–æ —É—Å–ª–æ–∂–Ω—è–µ—Ç —Ç–µ—Å—Ç
   - –ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≥–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –∫–æ–¥ —Ç–µ—Å—Ç–æ–≤ –Ω–∞ Python. –ù–∞—á–Ω–∏ —Å—Ä–∞–∑—É —Å import pytest.
"""
        return prompt
    
    def _clean_test_code(self, raw_code: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ —Ç–µ—Å—Ç–æ–≤."""
        if not raw_code:
            return ""
        
        lines = raw_code.split("\n")
        cleaned_lines: list[str] = []
        skip_until_code = False
        in_code_block = False
        
        for line in lines:
            stripped = line.strip()
            
            if stripped.startswith("```"):
                if not in_code_block:
                    in_code_block = True
                    skip_until_code = True
                else:
                    in_code_block = False
                continue
            
            if skip_until_code:
                if stripped.startswith("import") or stripped.startswith("from") or stripped.startswith("def test_"):
                    skip_until_code = False
                    cleaned_lines.append(line)
                continue
            
            if not cleaned_lines and (stripped.startswith("#") or not stripped or stripped.lower().startswith("–≤–æ—Ç")):
                continue
            
            cleaned_lines.append(line)
        
        cleaned = "\n".join(cleaned_lines).strip()
        
        if "def test_" not in cleaned and "def test" not in cleaned:
            logger.warning("‚ö†Ô∏è –í —Ç–µ—Å—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ—É–Ω–∫—Ü–∏–π test_*")
            return ""
        
        return cleaned


# === Factory —Ñ—É–Ω–∫—Ü–∏—è ===

def get_streaming_test_generator_agent(
    model: Optional[str] = None,
    temperature: float = 0.18
) -> StreamingTestGeneratorAgent:
    """–°–æ–∑–¥–∞—ë—Ç StreamingTestGeneratorAgent."""
    return StreamingTestGeneratorAgent(model=model, temperature=temperature)
