# Конфигурация для Cursor Killer
# Переменные окружения переопределяют значения из этого файла

# === Ollama Connection ===
# Настройки подключения к Ollama (локальный или удалённый)

[ollama]
# Хост Ollama API (по умолчанию локальный)
# Для удалённого сервера: "http://192.168.1.100:11434" или Tailscale "http://100.x.x.x:11434"
host = "http://localhost:11434"

# Таймаут подключения к Ollama в секундах
connect_timeout = 10

# Использовать удалённый Ollama (если true, игнорирует локальный)
# Полезно для переключения между локальным и удалённым без смены host
use_remote = false

[default]
# Модель для embeddings (RAG) — фиксированная
embedding_model = "nomic-embed-text"

# Остальные модели выбираются автоматически через SmartModelRouter
# на основе сложности задачи. Переопределите только если нужно 
# принудительно использовать конкретную модель:
# default_model = "qwen2.5-coder:7b"

# Максимальное количество итераций self-healing
max_iterations = 5

# Включить веб-поиск по умолчанию
enable_web = true

# Температура генерации (0.1 - 0.7, ниже = детерминированнее)
temperature = 0.25

# Порог предупреждения о количестве токенов
max_tokens_warning = 30000

# Директория для артефактов
output_dir = "output"

# Роевое использование моделей (будущее расширение)
enable_model_roster = false

# === Reasoning Models ===
# Настройки для моделей с встроенным chain-of-thought (DeepSeek-R1, QwQ)

[reasoning]
# Предпочитать reasoning модели для complex задач
# Если true, SmartModelRouter выберет DeepSeek-R1/QwQ вместо qwen2.5-coder для сложных задач
prefer_reasoning_models = true

# Показывать <think> блоки в UI
# Если true, рассуждения модели будут отображаться пользователю в реальном времени
show_thinking = true

# === Streaming ===
# Настройки real-time стриминга генерации

[streaming]
# Включить real-time стриминг (thinking + code)
enabled = true

# Размер чанка для стриминга thinking (символов)
thinking_chunk_size = 100

# Задержка между чанками thinking (мс) — для плавности UI
thinking_debounce_ms = 50

# Максимальное время на рассуждение (мс)
max_thinking_time_ms = 120000

# Использовать StreamingCoderAgent вместо CoderAgent
# Если false, используется синхронная генерация без стриминга
use_streaming_agents = true

# Минимальное качество для использования reasoning модели (0.0-1.0)
min_quality = 0.7

# Типы задач где предпочтительны reasoning модели
# complex задачи автоматически используют reasoning, но можно расширить
prefer_for_task_types = ["debug", "refactor", "analyze"]

# === Structured Output ===
# Настройки для Pydantic валидации ответов LLM

[structured_output]
# Включить structured output через JSON Schema + Pydantic
enabled = true

# Количество retry при ошибке валидации
max_retries = 2

# Агенты которые используют structured output
# По мере миграции сюда добавляются агенты
enabled_agents = ["intent", "debugger", "reflection"]

# Fallback на ручной парсинг если structured output не работает
fallback_to_manual_parsing = true

# === Code Retrieval (Few-Shot Examples) ===
# Phase 4: Поиск похожего кода для примеров

[code_retrieval]
# Включить code retrieval для few-shot примеров
enabled = true

# Источники для поиска примеров
# local — из текущего проекта, history — из успешных генераций, github — GitHub API
sources = ["local", "history"]

# Количество примеров для промпта
num_examples = 3

# Модель для embeddings (sentence-transformers)
embedding_model = "all-MiniLM-L6-v2"

# Путь к ChromaDB для индекса
chroma_path = ".chroma_code"

# GitHub токен для Code Search API (опционально)
# Без токена: 10 req/min, с токеном: 30 req/min
# github_token = "ghp_..."

# Интервал переиндексации проекта (минуты, 0 = отключено)
reindex_interval = 0

# Минимальное качество примера для включения (0.0-1.0)
min_quality = 0.5

# === Multi-Agent Debate ===
# Phase 5: Несколько критиков анализируют код

[multi_agent_debate]
# Включить multi-agent дебаты
enabled = false

# Максимум раундов дебатов
max_rounds = 3

# Минимальная сложность для дебатов (simple | medium | complex)
min_complexity = "medium"

# Рецензенты: security, performance, correctness
reviewers = ["security", "performance", "correctness"]

# Модель для рецензентов (пусто = использовать default)
reviewer_model = ""

# === Incremental Coding (Compiler-in-the-Loop) ===
# Phase 3: Инкрементальная генерация с немедленной валидацией
[incremental_coding]
# Включить инкрементальную генерацию
enabled = true

# Минимальная сложность для инкрементальной генерации
# simple | medium | complex
# При "complex" — используется только для сложных задач
min_complexity = "complex"

# Максимум попыток исправления на функцию
max_fix_attempts = 3

# Таймаут на валидацию одной функции (мс)
validation_timeout = 5000

# === LLM Generation Limits ===
# Лимиты токенов для разных этапов генерации
# Меньшие значения = быстрее, но менее детально
# Большие значения = медленнее, но более полно

[llm]
# Максимум токенов для планирования (короткий план)
tokens_planning = 256

# Максимум токенов для генерации тестов
tokens_tests = 2048

# Максимум токенов для генерации кода
# На M2/M1 использовать меньше для скорости (1024-2048)
tokens_code = 2048

# Максимум токенов для анализа/рефлексии
tokens_analysis = 1024

# Максимум токенов для классификации намерения (очень короткий ответ)
tokens_intent = 128

# Максимум токенов для анализа ошибок (debug)
tokens_debug = 2048

# Максимум токенов для критического анализа
tokens_critic = 512

# === Quality Thresholds ===
# Пороги качества для оценки результатов

[quality]
# Минимальный порог качества для успешного результата (0.0-1.0)
threshold = 0.7

# Минимальный порог уверенности агентов (0.0-1.0)
confidence_threshold = 0.75

# Порог для повторного запуска (если качество ниже, should_retry = true)
retry_threshold = 0.5

# === Web Search ===
# Настройки веб-поиска (Tavily, DuckDuckGo, Google)

[web_search]
# Таймаут веб-поиска в секундах
timeout = 10

# Максимальное количество результатов
max_results = 3

# API ключ для Tavily (опционально, также можно через TAVILY_API_KEY)
# tavily_api_key = "tvly-..."

# === RAG Settings ===
# Настройки Retrieval-Augmented Generation

[rag]
# Директория для хранения ChromaDB
persist_directory = ".chromadb"

# Название коллекции для памяти задач
memory_collection = "task_memory"

# Название коллекции для кодовой базы
code_collection = "code_knowledge"

# Минимальный порог схожести для результатов RAG (0.0-1.0)
similarity_threshold = 0.5

# Максимальное количество результатов из RAG
max_results = 5

# === Context Engine (Codebase Indexing) ===
# Настройки индексации кодовой базы

[context_engine]
# Включить индексацию кодовой базы
enabled = true

# Максимальный размер контекста в токенах
max_context_tokens = 4000

# Максимальный размер чанка в токенах
max_chunk_tokens = 500

# Директория для кэша индексов
cache_directory = ".context_cache"

# Расширения файлов по умолчанию для индексации
default_extensions = [".py"]

# === Interaction Settings ===
# Настройки режимов взаимодействия

[interaction]
# Режим по умолчанию: auto, chat, code
default_mode = "auto"

# Fallback модели для chat режима (используются только если SmartModelRouter
# не нашёл подходящую модель — редкий случай)
chat_model = "phi3:mini"
chat_model_fallback = "tinyllama:1.1b"

# Автоподтверждение workflow в режиме auto
auto_confirm = true

# Показывать процесс размышления агента
show_thinking = true

# Максимум сообщений до суммаризации контекста
max_context_messages = 20

# Сохранять диалоги на диск
persist_conversations = true

# Лимит токенов для chat ответов
tokens_chat = 2048

# === Hardware Limits ===
# Лимиты для автоматического выбора моделей

[hardware]
# Максимальный размер модели в GB (0 = без лимита, автоопределение)
# Установите явно если знаете лимит вашей VRAM
max_model_vram_gb = 0

# Разрешить использование моделей 30B+ для COMPLEX задач
allow_heavy_models = true

# Разрешить использование моделей 100B+ (требует много VRAM/RAM)
allow_ultra_models = false

# === LLM Timeouts ===
# Таймауты для разных этапов workflow (в секундах)
# Более сложные этапы (coder) получают больше времени

[timeouts]
# Определение намерения — увеличен для M2/медленных систем
intent = 120

# Планирование
planning = 120

# Исследование/контекст
research = 90

# Генерация тестов
testing = 120

# Генерация кода — самый долгий этап
coding = 180

# Валидация (запуск pytest/mypy) — не LLM, но может быть долгим
validation = 120

# Анализ ошибок
debug = 120

# Исправление кода
fixing = 150

# Рефлексия
reflection = 90

# Критический анализ
critic = 90

# Chat режим (увеличен для 7B моделей на CPU)
chat = 180

# Дефолтный таймаут для неизвестных этапов
default = 120

# === Task Persistence (Checkpoint System) ===
# Настройки сохранения состояния задач

[persistence]
# Включить систему checkpoint для сохранения состояния задач
enabled = true

# Директория для хранения checkpoint файлов
checkpoint_directory = ".task_checkpoints"

# Максимальный возраст checkpoint в часах (старые удаляются автоматически)
max_checkpoint_age_hours = 24

# Автоматически помечать задачи как paused при потере соединения
auto_pause_on_disconnect = true


# === Debug / Under The Hood ===
# Phase 7: Визуализация работы системы

[debug]
# Включить Under The Hood панель
under_the_hood_enabled = true

# Уровень логирования для UI (debug | info | warning | error)
log_level = "info"

# Максимум логов в памяти
max_logs_in_memory = 500

# Включить отслеживание tool calls
track_tool_calls = true

