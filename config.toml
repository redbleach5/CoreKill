# Конфигурация для Cursor Killer
# Переменные окружения переопределяют значения из этого файла

# === Ollama Connection ===
# Настройки подключения к Ollama (локальный или удалённый)

[ollama]
# Хост Ollama API (по умолчанию локальный)
# Для удалённого сервера: "http://192.168.1.100:11434" или Tailscale "http://100.x.x.x:11434"
# Переменные окружения OLLAMA_BASE_URL или OLLAMA_HOST переопределяют это значение
host = "http://localhost:11434"

# Таймаут подключения к Ollama в секундах
# Переменная окружения OLLAMA_TIMEOUT переопределяет это значение
connect_timeout = 10

# Таймаут для запросов к Ollama (в секундах, для connection pool)
# Переменная окружения OLLAMA_TIMEOUT переопределяет это значение
timeout = 300

# Размер пула соединений для Ollama
# Переменная окружения CONNECTION_POOL_SIZE переопределяет это значение
connection_pool_size = 10

# УДАЛЕНО: use_remote больше не нужен
# Просто меняйте host: "http://localhost:11434" для локального, или IP удалённого сервера

[default]
# Модель для embeddings (RAG) — фиксированная
embedding_model = "nomic-embed-text"

# Остальные модели выбираются автоматически через SmartModelRouter
# на основе сложности задачи. Переопределите только если нужно 
# принудительно использовать конкретную модель:
# default_model = "qwen2.5-coder:7b"

# Максимальное количество итераций self-healing
max_iterations = 5

# Включить веб-поиск по умолчанию
enable_web = true

# Температура генерации (0.1 - 0.7, ниже = детерминированнее)
temperature = 0.25

# Порог предупреждения о количестве токенов
max_tokens_warning = 30000

# Директория для артефактов
output_dir = "output"

# Роевое использование моделей (будущее расширение)
enable_model_roster = false

# === Reasoning Models ===
# Настройки для моделей с встроенным chain-of-thought (DeepSeek-R1, QwQ)

[reasoning]
# Предпочитать reasoning модели для complex задач
# Если true, SmartModelRouter выберет DeepSeek-R1/QwQ вместо qwen2.5-coder для сложных задач
prefer_reasoning_models = true

# Показывать <think> блоки в UI
# Если true, рассуждения модели будут отображаться пользователю в реальном времени
show_thinking = true

# === Streaming ===
# Настройки real-time стриминга генерации

[streaming]
# Включить real-time стриминг (thinking + code)
enabled = true

# Размер чанка для стриминга thinking (символов)
thinking_chunk_size = 100

# Задержка между чанками thinking (мс) — для плавности UI
thinking_debounce_ms = 50

# Максимальное время на рассуждение (мс)
max_thinking_time_ms = 1200000

# Использовать StreamingCoderAgent вместо CoderAgent
# Если false, используется синхронная генерация без стриминга
use_streaming_agents = true

# Минимальное качество для использования reasoning модели (0.0-1.0)
min_quality = 0.7

# Типы задач где предпочтительны reasoning модели
# complex задачи автоматически используют reasoning, но можно расширить
prefer_for_task_types = ["debug", "refactor", "analyze"]

# === Structured Output ===
# Настройки для Pydantic валидации ответов LLM

[structured_output]
# Включить structured output через JSON Schema + Pydantic
enabled = true

# Количество retry при ошибке валидации
max_retries = 2

# Агенты которые используют structured output
# По мере миграции сюда добавляются агенты
enabled_agents = ["intent", "debugger", "reflection"]

# Fallback на ручной парсинг если structured output не работает
fallback_to_manual_parsing = true

# === Code Retrieval (Few-Shot Examples) ===
# Phase 4: Поиск похожего кода для примеров

[code_retrieval]
# Включить code retrieval для few-shot примеров
enabled = true

# Источники для поиска примеров
# local — из текущего проекта, history — из успешных генераций, github — GitHub API
sources = ["local", "history"]

# Количество примеров для промпта
num_examples = 3

# Модель для embeddings (sentence-transformers)
embedding_model = "all-MiniLM-L6-v2"

# Путь к ChromaDB для индекса
chroma_path = ".chroma_code"

# GitHub токен для Code Search API (опционально)
# Без токена: 10 req/min, с токеном: 30 req/min
# github_token = "ghp_..."

# Интервал переиндексации проекта (минуты, 0 = отключено)
reindex_interval = 0

# Минимальное качество примера для включения (0.0-1.0)
min_quality = 0.5

# === Multi-Agent Debate ===
# Phase 5: Несколько критиков анализируют код

[multi_agent_debate]
# Включить multi-agent дебаты
enabled = false

# Максимум раундов дебатов
max_rounds = 3

# Минимальная сложность для дебатов (simple | medium | complex)
min_complexity = "medium"

# Рецензенты: security, performance, correctness
reviewers = ["security", "performance", "correctness"]

# Модель для рецензентов (пусто = использовать default)
reviewer_model = ""

# === Incremental Coding (Compiler-in-the-Loop) ===
# Phase 3: Инкрементальная генерация с немедленной валидацией
[incremental_coding]
# Включить инкрементальную генерацию
enabled = true

# Минимальная сложность для инкрементальной генерации
# simple | medium | complex
# При "complex" — используется только для сложных задач
min_complexity = "complex"

# Максимум попыток исправления на функцию
max_fix_attempts = 3

# Таймаут на валидацию одной функции (мс)
validation_timeout = 5000

# === LLM Generation Limits ===
# Лимиты токенов для разных этапов генерации
# Меньшие значения = быстрее, но менее детально
# Большие значения = медленнее, но более полно

[llm]
# Максимум токенов для планирования (короткий план)
tokens_planning = 256

# Максимум токенов для генерации тестов
tokens_tests = 2048

# Максимум токенов для генерации кода
# На M2/M1 использовать меньше для скорости (1024-2048)
tokens_code = 2048

# Максимум токенов для анализа/рефлексии
tokens_analysis = 1024

# Максимум токенов для классификации намерения (очень короткий ответ)
tokens_intent = 128

# Максимум токенов для анализа ошибок (debug)
tokens_debug = 2048

# Максимум токенов для критического анализа
tokens_critic = 512

# === Prompt Length Limits ===
# Лимиты длины для обрезки контента в промптах (символов)
# Предотвращает переполнение контекста и дублирование кода

[prompt_limits]
# Максимальная длина тестов в промпте генерации кода
max_tests_length = 2000

# Максимальная длина тестов в промпте исправления кода
max_tests_length_fix = 1000

# Максимальная длина контекста в промпте
max_context_length = 1000

# Максимальная длина ошибок валидации в промпте
max_validation_errors_length = 300

# === Quality Thresholds ===
# Пороги качества для оценки результатов

[quality]
# Минимальный порог качества для успешного результата (0.0-1.0)
threshold = 0.7

# Минимальный порог уверенности агентов (0.0-1.0)
confidence_threshold = 0.75

# Порог для повторного запуска (если качество ниже, should_retry = true)
retry_threshold = 0.5

# Минимальное качество модели для каждой сложности задачи (для ModelRouter)
# SIMPLE: любая модель от 1.5B
min_quality_simple = 0.3
# MEDIUM: минимум 7B или хорошая coder
min_quality_medium = 0.55
# COMPLEX: минимум 7B coder или 13B+
min_quality_complex = 0.7

# === Web Search ===
# Настройки веб-поиска (Tavily, DuckDuckGo, Google)

[web_search]
# Таймаут веб-поиска в секундах
timeout = 10

# Максимальное количество результатов
# Увеличено для более глубокого поиска лучших практик
max_results = 5

# API ключ для Tavily (опционально, также можно через TAVILY_API_KEY)
# tavily_api_key = "tvly-..."
# Для максимальной эффективности рекомендуется установить TAVILY_API_KEY

# === RAG Settings ===
# Настройки Retrieval-Augmented Generation

[rag]
# Директория для хранения ChromaDB
persist_directory = ".chromadb"

# Название коллекции для памяти задач
memory_collection = "task_memory"

# Название коллекции для кодовой базы
code_collection = "code_knowledge"

# Минимальный порог схожести для результатов RAG (0.0-1.0)
similarity_threshold = 0.5

# Максимальное количество результатов из RAG
max_results = 5

# === Context Engine (Codebase Indexing) ===
# Настройки индексации кодовой базы

[context_engine]
# Включить индексацию кодовой базы
enabled = true

# Максимальный размер контекста в токенах
max_context_tokens = 4000

# Максимальный размер чанка в токенах
max_chunk_tokens = 500

# Директория для кэша индексов
cache_directory = ".context_cache"

# Расширения файлов по умолчанию для индексации
default_extensions = [".py"]

# === Interaction Settings ===
# Настройки режимов взаимодействия

[interaction]
# Режим по умолчанию: auto, chat, code
default_mode = "auto"

# Fallback модели для chat режима (используются только если SmartModelRouter
# не нашёл подходящую модель — редкий случай)
chat_model = "phi3:mini"
chat_model_fallback = "tinyllama:1.1b"

# Автоподтверждение workflow в режиме auto
auto_confirm = true

# Показывать процесс размышления агента
show_thinking = true

# Максимум сообщений до суммаризации контекста
max_context_messages = 20

# Сохранять диалоги на диск
persist_conversations = true

# Лимит токенов для chat ответов
tokens_chat = 2048

# === Hardware Limits ===
# Лимиты для автоматического выбора моделей

[hardware]
# Максимальный размер модели в GB (0 = без лимита, автоопределение)
# Установите явно если знаете лимит вашей VRAM
max_model_vram_gb = 0

# Разрешить использование моделей 30B+ для COMPLEX задач
allow_heavy_models = true

# Разрешить использование моделей 100B+ (требует много VRAM/RAM)
allow_ultra_models = false

# === LLM Timeouts ===
# Таймауты для разных этапов workflow (в секундах)
# Более сложные этапы (coder) получают больше времени

[timeouts]
# Определение намерения — увеличен для M2/медленных систем
intent = 120

# Планирование
planning = 120

# Исследование/контекст
research = 90

# Генерация тестов
testing = 120

# Генерация кода — самый долгий этап
coding = 180

# Валидация (запуск pytest/mypy) — не LLM, но может быть долгим
validation = 120

# Анализ ошибок
debug = 120

# Исправление кода
fixing = 150

# Рефлексия
reflection = 90

# Критический анализ
critic = 90

# Chat режим
# Для SIMPLE задач (greeting, help, обучение) используем короткий timeout
# Для MEDIUM/COMPLEX может быть дольше, но максимум 60с для простых запросов
chat = 60

# Дефолтный таймаут для неизвестных этапов
default = 120

# === Task Persistence (Checkpoint System) ===
# Настройки сохранения состояния задач

[persistence]
# Включить систему checkpoint для сохранения состояния задач
enabled = true

# Директория для хранения checkpoint файлов
checkpoint_directory = ".task_checkpoints"

# Максимальный возраст checkpoint в часах (старые удаляются автоматически)
max_checkpoint_age_hours = 24

# Автоматически помечать задачи как paused при потере соединения
auto_pause_on_disconnect = true


# === Agent Resource Management ===
# Контроль ресурсов для предотвращения перегрузки системы

[agent_resources]
# Максимальное количество одновременных агентов
# Ограничивает количество параллельных вызовов агентов для предотвращения
# перегрузки системы и исчерпания ресурсов
max_concurrent_agents = 5

# === Code Security ===
# Проверка безопасности сгенерированного кода

[code_security]
# Включить проверку безопасности
enabled = true

# Строгий режим (блокирует код с опасными операциями)
# Если false, только предупреждает, но не блокирует
strict_mode = false

# Блокировать опасные импорты (os, subprocess, etc.)
block_dangerous_imports = true

# Блокировать опасные функции (eval, exec, etc.)
block_dangerous_functions = true

# Блокировать системные вызовы (os.system, subprocess.run, etc.)
block_system_calls = true

# === Code Style Configuration ===
# Настройки стиля генерируемого кода

[code_style]
# Стиль docstrings: google, numpy, sphinx, russian (по умолчанию)
docstring_style = "russian"

# Язык docstrings: ru, en
docstring_language = "ru"

# Стиль именования: snake_case, camelCase, PascalCase
naming_style = "snake_case"

# Требовать type hints
require_type_hints = true

# Требовать docstrings для публичного API
require_docstrings = true

# Следовать PEP8
follow_pep8 = true

# Максимальная длина строки (символов)
max_line_length = 88

# === Circuit Breaker ===
# Защита от каскадных сбоев при повторяющихся ошибках агентов

[circuit_breaker]
# Порог ошибок для открытия circuit breaker
# После этого количества ошибок circuit временно отключает вызовы
failure_threshold = 5

# Время ожидания перед попыткой восстановления (секунды)
# После этого времени circuit переходит в HALF_OPEN для тестирования
recovery_timeout = 30.0

# Количество успешных вызовов для закрытия circuit (в HALF_OPEN)
# После этого количества успешных вызовов circuit закрывается
success_threshold = 2

# === Fast Advisor (Быстрые консультации) ===
# Модуль для асинхронных консультаций с легкими reasoning моделями
# Решает проблему долгих ответов, предоставляя быстрые советы параллельно

[fast_advisor]
# Включить быстрые консультации
enabled = true

# Модель для консультаций (None = автовыбор легкой reasoning модели)
# Рекомендуется: phi3:mini, gemma:2b, tinyllama:1.1b
model = ""

# Таймаут для консультации (секунды)
# Короткий таймаут = быстрее, но может не успеть ответить
timeout = 10

# Включить кэширование ответов
enable_cache = true

# Время жизни кэша (секунды)
cache_ttl = 3600

# === Performance / UI Smoothness ===
# Настройки задержек для плавности UI

[performance]
# Включить задержки для плавности UI (по умолчанию true для локальной разработки)
# В продакшене рекомендуется установить false для максимальной производительности
enable_ui_smoothness_delays = true

# Задержка между SSE событиями (секунды)
# 0.02 = 20ms (по умолчанию для плавности)
# 0.0 = без задержек (максимальная производительность)
ui_delay_seconds = 0.02

# Задержка для критических событий (final_result и т.д.)
critical_delay_seconds = 0.2

# === Debug / Under The Hood ===
# Phase 7: Визуализация работы системы

[debug]
# Включить Under The Hood панель
under_the_hood_enabled = true

# Уровень логирования для UI (debug | info | warning | error)
# info - нормальный режим (рекомендуется для production)
# debug - детальное логирование (только для отладки, создает много логов)
log_level = "info"

# Максимум логов в памяти
max_logs_in_memory = 500

# Включить отслеживание tool calls
track_tool_calls = true

# === Autonomous Improver (Автономное улучшение проекта) ===
# Модуль для фонового анализа и предложения улучшений проекта

[autonomous_improver]
# Включить автономное улучшение проекта
# Для тестирования установите enabled = true
enabled = false

# Путь к проекту для анализа (пустая строка = текущая директория)
# Установлен на текущий проект для максимальной оптимизации
project_path = ""

# Модель для анализа (пустая строка = автовыбор легкой модели)
# Автовыбор обеспечит использование всех доступных моделей с fallback
model = ""

# Минимальная уверенность для предложения (0.0-1.0)
# 0.9 = предлагать при 90%+ уверенности (более агрессивный режим для теста)
# Это позволит найти больше улучшений за 4 часа
min_confidence = 0.9

# Максимум файлов для анализа за один цикл
# Увеличено для более быстрого покрытия проекта за 4 часа
max_files_per_cycle = 15

# Интервал между циклами анализа (секунды)
# 180 = 3 минуты (более частые циклы для интенсивного теста)
# Это позволит выполнить ~80 циклов за 4 часа
cycle_interval = 180

# Максимальное количество файлов для параллельного анализа
# 5 = максимальная скорость при сохранении стабильности
max_parallel = 5

