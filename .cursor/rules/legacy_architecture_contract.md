# ВАЖНО: КОНТРАКТ, А НЕ КОД
- НЕ описывает готовую архитектуру
- НЕ должен быть воспроизведён целиком

Назначение:
Выделить ПРОВЕРЕННЫЕ идеи из неудачных проектов
и реализовать их ЗАНОВО, минимально и инкрементально.

Запрещено:
- копировать старые реализации
- переносить сложность без необходимости
- реализовывать всё сразу

> Выделение сильных решений, которые можно применить в cursor-killer

---

## Context Engine (CORE26)

**Как решает проблемы и слабые места нашего текущего проекта:**
- Проблема: Небольшие модели (7B) не могут обработать большие кодовые базы (50-500K токенов) в контексте (4-8K)
- Проблема: Наш Researcher просто ищет релевантные чанки через RAG, но не учитывает структуру кода и зависимости
- Решение: Создаёт граф зависимостей кода (AST парсинг) → умное разбиение на чанки → иерархические сводки → оценка релевантности → оптимальный контекст в пределах лимита токенов

**Как реализовано концептуально:**
1. **CodeGraph** — парсит Python AST, строит граф зависимостей (imports, inheritance, calls), вычисляет важность (PageRank-like)
2. **SmartChunker** — разбивает код, уважая границы функций/классов, создаёт signature-only чанки для больших классов
3. **HierarchicalSummarizer** — создаёт многоуровневые сводки: проект → модуль → класс → функция
4. **RelevanceScorer** — BM25 + расширение запроса синонимами + структурная важность + семантическая близость
5. **ContextComposer** — собирает оптимальный контекст с разными стратегиями (GREEDY, DIVERSE, HIERARCHICAL, FOCUSED, BREADTH)
6. **ContextEngine** — объединяет всё, кэширует индексы проектов

**Почему это сильное решение:**
- Позволяет маленьким моделям понимать огромные проекты
- Сохраняет структуру и зависимости, а не просто текстовый поиск
- Индексация один раз → мгновенный доступ к контексту
- Гибкие стратегии композиции под разные задачи

**Где применять в новом проекте:**
- Интегрировать в `agents/researcher.py` — вместо простого RAG использовать Context Engine для анализа существующего кода
- Для режима `modify`/`debug` — когда пользователь указывает файл, использовать Context Engine для понимания зависимостей
- В `utils/file_context.py` — расширить для работы с графом зависимостей
- Создать новый модуль `infrastructure/context_engine.py` с упрощённой версией

**Какие риски при повторной реализации:**
- Сложность: AST парсинг требует хорошего понимания структуры языков
- Производительность: Индексация больших проектов может быть медленной (нужен кэш)
- Память: Граф зависимостей может занимать много памяти
- Поддержка: Нужно поддерживать парсинг для разных языков (сейчас только Python)

---

## Hierarchical Summarization (CORE26)

**Как решает проблемы и слабые места нашего текущего проекта:**
- Проблема: Наш Researcher возвращает сырые чанки кода, которые могут быть неструктурированными
- Проблема: Нет понимания общей структуры проекта перед детальным анализом
- Решение: Создаёт иерархические сводки (проект → модуль → класс → функция), которые дают обзор без перегрузки контекста

**Как реализовано концептуально:**
- Многоуровневая сводка: каждый уровень суммирует предыдущий
- Использует LLM для генерации сводок (но можно закешировать)
- Сохраняет важные детали (API, сигнатуры) и убирает реализации
- Комбинируется с детальными чанками по запросу

**Почему это сильное решение:**
- Даёт "big picture" перед деталями
- Экономит токены — можно передать обзор + несколько ключевых деталей вместо всего кода
- Сохраняет структуру проекта в понятном виде

**Где применять в новом проекте:**
- В `agents/planner.py` — использовать сводки проекта для лучшего планирования
- В `agents/researcher.py` — добавить этап создания сводок перед детальным поиском
- Создать `utils/summarizer.py` — простой вариант для генерации сводок через LLM

**Какие риски при повторной реализации:**
- LLM зависимость: Генерация сводок требует дополнительных вызовов модели
- Кэширование: Нужно кэшировать сводки, иначе каждый раз будет дорого
- Актуальность: Сводки могут устаревать при изменении кода (нужна инвалидация)

---

## Intelligent Model Router (AILLM + CORE26)

**Как решает проблемы и слабые места нашего текущего проекта:**
- Проблема: Мы используем одну модель для всех задач (codellama:7b), хотя есть более подходящие модели для разных задач
- Проблема: Нет учёта производительности моделей для разных типов задач
- Решение: Динамический выбор модели на основе типа задачи, исторических метрик, и аппаратного обеспечения

**Как реализовано концептуально:**
1. **ModelProfile** — профиль модели (размер, возможности, базовая скорость/качество)
2. **Capability Detection** — автоопределение возможностей по названию (code_generation, reasoning, fast_response)
3. **Performance Tracker** — отслеживание успешности/скорости для каждого типа задачи
4. **Scoring System** — взвешенная оценка моделей (качество × скорость × исторические метрики)
5. **Hardware Awareness** — выбор модели с учётом доступной памяти/GPU

**Почему это сильное решение:**
- Использует сильные стороны каждой модели (phi3:mini для быстрых ответов, codellama для кода)
- Адаптивное обучение — система учится, какие модели лучше для каких задач
- Экономия ресурсов — не использует большие модели для простых задач

**Где применять в новом проекте:**
- Создать `infrastructure/model_router.py` — упрощённая версия
- В `infrastructure/local_llm.py` — добавить выбор модели на основе задачи
- Интегрировать в каждый агент — Intent использует легкую модель, Coder — код-модель
- В `utils/config.py` — добавить конфигурацию возможностей моделей

**Какие риски при повторной реализации:**
- Усложнение: Нужно поддерживать профили для всех моделей
- Нестабильность: Модели могут вести себя иначе, чем ожидается по профилю
- Производительность: Дополнительные проверки и расчёты могут замедлить систему

---

## Learning System с персистентностью (AILLM + CORE26)

**Как решает проблемы и слабые места нашего текущего проекта:**
- Проблема: Наш `agents/memory.py` хранит только прошлые задачи, но не анализирует паттерны успешности
- Проблема: Нет систематического обучения на опыте — мы просто сохраняем, но не улучшаем промпты
- Решение: SQLite база для хранения рефлексий, успешных паттернов, типичных ошибок, и адаптивных рекомендаций

**Как реализовано концептуально:**
- **Reflection History** — таблица результатов рефлексии с оценками (completeness, correctness, quality)
- **Successful Solutions** — таблица успешных решений для few-shot learning
- **Error Patterns** — анализ типичных ошибок и способов их исправления
- **Agent Stats** — статистика по каждому агенту (success rate, avg quality, common issues)
- **Prompt Recommendations** — автоматические рекомендации по улучшению промптов

**Почему это сильное решение:**
- Персистентность — опыт сохраняется между сессиями
- Аналитика — можно увидеть, где система слаба, и улучшить
- Few-shot learning — успешные решения используются как примеры
- Адаптация — система автоматически улучшает промпты на основе опыта

**Где применять в новом проекте:**
- Расширить `agents/memory.py` — добавить анализ паттернов и рекомендации
- Создать `utils/learning_system.py` — интегрировать с существующей памятью
- В `agents/reflection.py` — сохранять результаты рефлексии в Learning System
- В `agents/planner.py` и `agents/researcher.py` — использовать рекомендации из Learning System

**Какие риски при повторной реализации:**
- Сложность SQL схемы: Нужно правильно спроектировать таблицы
- Производительность: Запросы к SQLite могут быть медленными при большом количестве данных
- Миграции: Изменение схемы требует миграций
- Качество данных: Плохие данные (неправильные рефлексии) могут испортить обучение

---

## Reflection Mixin с самокоррекцией (AILLM + CORE26)

**Как решает проблемы и слабые места нашего текущего проекта:**
- Проблема: Наш `agents/reflection.py` только оценивает результат, но не исправляет автоматически
- Проблема: Если качество низкое, мы просто сообщаем об этом, но не пытаемся улучшить
- Решение: Агенты автоматически анализируют результат, выявляют проблемы, и исправляют их до 2-3 попыток

**Как реализовано концептуально:**
- **ReflectionMixin** — добавляет метод `reflect_on_result()` к любому агенту
- **Quality Assessment** — оценивает completeness, correctness, quality (0-100)
- **Issue Detection** — выявляет конкретные проблемы в результате
- **Self-Correction Loop** — если качество < threshold, автоматически исправляет с учётом выявленных проблем
- **Learning Integration** — сохраняет опыт рефлексии в Learning System

**Почему это сильное решение:**
- Автоматизация — не нужно вручную исправлять ошибки
- Итеративное улучшение — качество повышается с каждой попыткой
- Обучаемость — система запоминает, какие исправления работают

**Где применять в новом проекте:**
- Расширить `agents/reflection.py` — добавить самокоррекцию
- Применить как миксин к `agents/coder.py` и `agents/test_generator.py`
- В workflow после генерации кода/тестов — автоматически проверять и исправлять

**Какие риски при повторной реализации:**
- Бесконечные циклы: Может зациклиться, если исправления не помогают (нужен лимит попыток)
- Переисправление: Может ухудшить результат при попытке исправить (нужен threshold для остановки)
- LLM зависимость: Рефлексия требует дополнительных вызовов модели

---

## Performance Tracker (AILLM + CORE26)

**Как решает проблемы и слабые места нашего текущего проекта:**
- Проблема: Мы не отслеживаем, какие модели быстрее/качественнее для разных задач
- Проблема: Нет метрик для оптимизации выбора моделей
- Решение: Система отслеживает время выполнения, успешность, качество для каждой модели и типа задачи

**Как реализовано концептуально:**
- **Request-level Metrics** — сохраняет каждое использование модели (task_type, model, success, time, tokens)
- **SQLite Persistence** — метрики сохраняются между сессиями
- **Aggregated Stats** — вычисляет средние, успешность, рекомендации
- **Model Recommendations** — рекомендует лучшие модели для каждого типа задачи на основе истории

**Почему это сильное решение:**
- Данно-ориентированный подход — решения основаны на реальных метриках
- Адаптация — система автоматически улучшает выбор моделей
- Оптимизация — можно найти самые быстрые/качественные модели для конкретных задач

**Где применять в новом проекте:**
- Создать `utils/performance_tracker.py` — интегрировать с LocalLLM
- В `infrastructure/local_llm.py` — отслеживать каждое использование модели
- В `infrastructure/model_router.py` — использовать метрики для выбора модели
- В UI — показывать статистику производительности моделей

**Какие риски при повторной реализации:**
- Overhead: Отслеживание каждого запроса может замедлить систему
- Точность: Метрики могут быть неточными (время включает network latency)
- Размер БД: При большом количестве запросов база может расти быстро

---

## Hardware-aware Model Selection (CORE26)

**Как решает проблемы и слабые места нашего текущего проекта:**
- Проблема: Мы используем фиксированные модели, не учитывая возможности оборудования
- Проблема: На слабом оборудовании система может быть медленной или не работать
- Решение: Автоматическое определение класса оборудования (MINIMAL/LIGHT/MEDIUM/HEAVY/EXTREME/CLUSTER) и выбор подходящих моделей

**Как реализовано концептуально:**
- **Hardware Detection** — определяет CPU, RAM, GPU, OS
- **Hardware Classes** — категоризирует оборудование (MINIMAL → <8GB RAM, EXTREME → 48GB+ VRAM)
- **Model Limits** — для каждого класса определяет максимальные размеры моделей
- **Automatic Selection** — выбирает модели в пределах возможностей оборудования

**Почему это сильное решение:**
- Универсальность — работает на любом оборудовании
- Оптимизация — использует максимум возможностей оборудования
- User-friendly — пользователю не нужно вручную настраивать модели

**Где применять в новом проекте:**
- Создать `utils/hardware.py` — упрощённая версия детектора
- В `utils/config.py` — добавить автоопределение оптимальной модели
- В `agents/*` — использовать для выбора модели при инициализации
- В UI — показывать информацию об оборудовании и рекомендуемых моделях

**Какие риски при повторной реализации:**
- Точность определения: Может неправильно определить класс оборудования
- Несовместимость: Разные ОС/архитектуры могут требовать разной логики
- Статичность: Если оборудование изменится (например, запуск на GPU), нужно переопределять

---

## Uncertainty Detection + Auto Search (CORE26)

**Как решает проблемы и слабые места нашего текущего проекта:**
- Проблема: Наш Researcher делает веб-поиск только при низкой уверенности RAG, но не при низкой уверенности LLM
- Проблема: Модель может дать ответ, но быть неуверенной — мы не проверяем это
- Решение: Автоматическое определение неуверенных ответов модели и дополнение через веб-поиск

**Как реализовано концептуально:**
- **UncertaintySearchMixin** — добавляет проверку неуверенности к агентам
- **Confidence Detection** — анализирует ответ модели на признаки неуверенности (фразы типа "I'm not sure", "I think", "may be")
- **Auto Web Search** — если уверенность низкая, автоматически ищет дополнительную информацию
- **Response Enhancement** — дополняет ответ актуальной информацией из веб-поиска

**Почему это сильное решение:**
- Автоматизация — не нужно вручную проверять уверенность
- Улучшение качества — неуверенные ответы автоматически улучшаются
- Гибкость — работает для любых агентов через миксин

**Где применять в новом проекте:**
- Создать `agents/uncertainty_search.py` — миксин для определения неуверенности
- Применить к `agents/planner.py` и `agents/coder.py`
- Интегрировать с существующим `infrastructure/web_search.py`

**Какие риски при повторной реализации:**
- False positives: Может неправильно определить неуверенность (модель может быть уверена, но ошибаться)
- Производительность: Дополнительный веб-поиск замедляет ответ
- Стоимость: Если используется платный веб-поиск, может быть дорого

---

## Сводка: Приоритеты внедрения

### Высокий приоритет:
1. **Context Engine (упрощённая версия)** — критично для работы с большими проектами
2. **Intelligent Model Router** — существенно улучшит качество и скорость
3. **Reflection с самокоррекцией** — автоматически улучшит результаты

### Средний приоритет:
4. **Learning System с персистентностью** — улучшит долгосрочное обучение
5. **Performance Tracker** — оптимизирует выбор моделей
6. **Hardware-aware Selection** — улучшит универсальность

### Низкий приоритет:
7. **Hierarchical Summarization** — nice-to-have для больших проектов
8. **Uncertainty Detection** — можно реализовать позже

---

## Вероятные причины деградации проектов

### AILLM:
1. **Сложность архитектуры** — слишком много компонентов (7 агентов, AutoML, мультимодальность) → сложно поддерживать
2. **Много провайдеров** — поддержка OpenAI, Anthropic, Ollama усложняет код
3. **Недостаточное тестирование** — большой проект без достаточного покрытия тестами
4. **Слабая документация** — множество компонентов без понятной документации

### CORE26:
1. **Слишком амбициозно** — Context Engine очень сложен для реализации и поддержки
2. **Асинхронность** — полная async архитектура сложна для понимания
3. **Зависимости** — много зависимостей (Ray, FAISS, aiosqlite) → сложно развернуть
4. **Неполная реализация** — много компонентов начаты, но не завершены

### Общие проблемы:
1. **Feature Creep** — слишком много функций одновременно
2. **Недостаточная фокусировка** — попытка сделать всё сразу
3. **Слабая валидация** — нет строгой проверки качества на раннем этапе
4. **Отсутствие инкрементальности** — большой рефакторинг вместо постепенного улучшения

---

## Рекомендации для cursor-killer

1. **Начните с малого** — внедряйте паттерны постепенно, не все сразу
2. **Фокус на качестве** — лучше сделать меньше, но качественно
3. **Тестирование** — покрытие тестами ≥85% обязательно
4. **Инкрементальность** — каждое улучшение должно быть независимым
5. **Документация** — документируйте каждое решение
6. **Простота > Сложность** — упрощённые версии лучше сложных, если они работают
