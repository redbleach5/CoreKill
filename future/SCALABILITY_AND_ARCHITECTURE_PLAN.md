# ĞŸĞ»Ğ°Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹

## Ğ”Ğ°Ñ‚Ğ°: 2026-01-21

---

## ğŸ¯ Ğ¦ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

### Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ
- **ĞĞ´Ğ½Ğ¾Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼:** Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾
- **Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°:** Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾
- **In-memory ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ:** ĞĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°Ğ¼Ğ¸
- **Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹:** ĞŸÑ€Ğ¸Ğ²ÑĞ·ĞºĞ° Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğµ Ñ Ollama
- **ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹:** ĞĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡

### Ğ¦ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ (Ñ‡ĞµÑ€ĞµĞ· 6-12 Ğ¼ĞµÑÑÑ†ĞµĞ²)
- **100+ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹**
- **1000+ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ´ĞµĞ½ÑŒ**
- **< 5s latency Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡**
- **99.9% uptime**
- **Horizontal scaling** (Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ²)

---

## ğŸ“Š Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ vs Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

### Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ (Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ°Ñ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Single Process                  â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Frontend â”‚ -> â”‚ FastAPI  â”‚ ->â”‚ Ollama â”‚ â”‚
â”‚  â”‚  (Vite)  â”‚    â”‚ Backend  â”‚   â”‚ (Local)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚                     â”‚
â”‚                        v                     â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                   â”‚ ChromaDBâ”‚               â”‚
â”‚                   â”‚ (Local) â”‚               â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ«:
- ĞĞ´Ğ¸Ğ½ ÑĞµÑ€Ğ²ĞµÑ€ = single point of failure
- ĞĞµÑ‚ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
- LLM Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹
- ĞĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ°
```

### Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ (Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ + Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Load Balancer (nginx)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Gateway Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  API     â”‚  â”‚  API     â”‚  â”‚  API     â”‚  (Horizontal      â”‚
â”‚  â”‚ Instance â”‚  â”‚ Instance â”‚  â”‚ Instance â”‚   scaling)        â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Message Queue (Redis)                   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Priority â”‚  â”‚ Standard â”‚  â”‚   Batch  â”‚  (Task queues)   â”‚
â”‚  â”‚  Queue   â”‚  â”‚  Queue   â”‚  â”‚  Queue   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Worker Pool                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Worker 1 â”‚  â”‚ Worker 2 â”‚  â”‚ Worker N â”‚  (Auto-scale)    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Backend Services                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama  â”‚  â”‚  Ollama  â”‚  â”‚  ChromaDB â”‚  â”‚  Redis   â”‚  â”‚
â”‚  â”‚ Server 1 â”‚  â”‚ Server 2 â”‚  â”‚ Cluster   â”‚  â”‚  Cache   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ĞŸĞ Ğ•Ğ˜ĞœĞ£Ğ©Ğ•Ğ¡Ğ¢Ğ’Ğ:
âœ… Horizontal scaling
âœ… Load balancing
âœ… Task queues Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸
âœ… Fault tolerance
âœ… Distributed caching
âœ… Independent scaling ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
```

---

## ğŸ—ï¸ ĞŸĞ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸

### Ğ¤Ğ°Ğ·Ğ° 1: ĞÑ‡ĞµÑ€ĞµĞ´Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ (1-2 Ğ½ĞµĞ´ĞµĞ»Ğ¸)

**Ğ¦ĞµĞ»ÑŒ:** Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ API Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ·Ğ°Ğ´Ğ°Ñ‡

**Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸:** 
- **Redis** (message broker)
- **Celery** Ğ¸Ğ»Ğ¸ **RQ** (task queue)

**Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:**

```python
# infrastructure/task_queue.py
"""Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡."""

from celery import Celery
from kombu import Queue, Exchange
from typing import Dict, Any
import redis

app = Celery('cursor_killer')

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ
app.conf.update(
    broker_url='redis://localhost:6379/0',
    result_backend='redis://localhost:6379/1',
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    
    # ĞÑ‡ĞµÑ€ĞµĞ´Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸
    task_queues=[
        Queue('priority', Exchange('priority'), routing_key='priority'),
        Queue('standard', Exchange('standard'), routing_key='standard'),
        Queue('batch', Exchange('batch'), routing_key='batch'),
    ],
    
    # Routing
    task_routes={
        'tasks.generate_code_priority': {'queue': 'priority'},
        'tasks.generate_code': {'queue': 'standard'},
        'tasks.batch_process': {'queue': 'batch'},
    },
    
    # Limits
    task_time_limit=600,  # 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚
    task_soft_time_limit=540,  # 9 Ğ¼Ğ¸Ğ½ÑƒÑ‚ (soft limit)
    worker_max_tasks_per_child=100,  # ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ¾ÑĞ»Ğµ 100 Ğ·Ğ°Ğ´Ğ°Ñ‡
)

# Task definitions
@app.task(name='tasks.generate_code', bind=True)
def generate_code_task(self, task_id: str, params: Dict[str, Any]):
    """Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°."""
    from infrastructure.workflow_graph import create_workflow
    from infrastructure.task_checkpointer import TaskCheckpointer
    
    try:
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ workflow
        workflow = create_workflow()
        
        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑ
        self.update_state(state='PROCESSING', meta={'stage': 'intent'})
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼
        result = workflow.invoke(params)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
        checkpointer = TaskCheckpointer()
        checkpointer.save_checkpoint(task_id, result, status='completed')
        
        return {'status': 'success', 'result': result}
        
    except Exception as e:
        # Retry Ñ exponential backoff
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

@app.task(name='tasks.generate_code_priority')
def generate_code_priority(task_id: str, params: Dict[str, Any]):
    """ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°."""
    return generate_code_task.apply_async(
        args=[task_id, params],
        queue='priority'
    )
```

**API Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ:**

```python
# backend/routers/agent.py

from infrastructure.task_queue import generate_code_task
from infrastructure.task_checkpointer import TaskCheckpointer

@router.post("/api/tasks")
async def create_task(request: TaskRequest):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ."""
    
    # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ID
    task_id = str(uuid.uuid4())
    
    # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ
    celery_task = generate_code_task.apply_async(
        args=[task_id, request.dict()],
        task_id=task_id
    )
    
    return {
        "task_id": task_id,
        "status": "queued",
        "check_url": f"/api/tasks/{task_id}"
    }

@router.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."""
    from celery.result import AsyncResult
    
    result = AsyncResult(task_id)
    
    if result.state == 'PENDING':
        return {"status": "queued"}
    elif result.state == 'PROCESSING':
        return {"status": "processing", "meta": result.info}
    elif result.state == 'SUCCESS':
        checkpointer = TaskCheckpointer()
        checkpoint = checkpointer.load_checkpoint(task_id)
        return {"status": "completed", "result": checkpoint}
    else:
        return {"status": "failed", "error": str(result.info)}
```

**Frontend polling:**

```tsx
// frontend/src/hooks/useTaskPolling.ts
import { useState, useEffect } from 'react'

export function useTaskPolling(taskId: string, interval = 2000) {
  const [status, setStatus] = useState<'queued' | 'processing' | 'completed' | 'failed'>('queued')
  const [result, setResult] = useState(null)
  
  useEffect(() => {
    const poll = async () => {
      const res = await fetch(`/api/tasks/${taskId}`)
      const data = await res.json()
      
      setStatus(data.status)
      
      if (data.status === 'completed') {
        setResult(data.result)
      }
    }
    
    const timer = setInterval(poll, interval)
    
    // ĞÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ polling Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸
    if (status === 'completed' || status === 'failed') {
      clearInterval(timer)
    }
    
    return () => clearInterval(timer)
  }, [taskId, status, interval])
  
  return { status, result }
}
```

**Checklist:**
- [ ] Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Redis
- [ ] ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Celery/RQ
- [ ] Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ task definitions
- [ ] API endpoints Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡
- [ ] Frontend polling
- [ ] ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹ (Flower Ğ´Ğ»Ñ Celery)
- [ ] Graceful shutdown workers

---

### Ğ¤Ğ°Ğ·Ğ° 2: Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (1 Ğ½ĞµĞ´ĞµĞ»Ñ)

**Ğ¦ĞµĞ»ÑŒ:** Shared cache Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ°Ğ¼Ğ¸ API

```python
# infrastructure/distributed_cache.py
"""Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ ĞºÑÑˆ Ñ‡ĞµÑ€ĞµĞ· Redis."""

import redis
import json
import hashlib
from typing import Optional, Any
from functools import wraps

class DistributedCache:
    """Redis-based distributed cache."""
    
    def __init__(self, host='localhost', port=6379, db=2):
        self.redis = redis.Redis(
            host=host,
            port=port,
            db=db,
            decode_responses=True
        )
    
    def get(self, key: str) -> Optional[Any]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· ĞºÑÑˆĞ°."""
        value = self.redis.get(key)
        return json.loads(value) if value else None
    
    def set(self, key: str, value: Any, ttl: int = 3600):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºÑÑˆ."""
        self.redis.setex(
            key,
            ttl,
            json.dumps(value)
        )
    
    def delete(self, key: str):
        """Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· ĞºÑÑˆĞ°."""
        self.redis.delete(key)
    
    def invalidate_pattern(self, pattern: str):
        """Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ²ÑĞµ ĞºĞ»ÑÑ‡Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñƒ."""
        for key in self.redis.scan_iter(match=pattern):
            self.redis.delete(key)

# Ğ”ĞµĞºĞ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
def cached(ttl: int = 3600, key_prefix: str = ""):
    """Ğ”ĞµĞºĞ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸."""
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ»ÑÑ‡ ĞºÑÑˆĞ°
            cache_key = _generate_cache_key(key_prefix, func.__name__, args, kwargs)
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºÑÑˆ
            cache = DistributedCache()
            cached_value = cache.get(cache_key)
            
            if cached_value is not None:
                return cached_value
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ¸ ĞºÑÑˆĞ¸Ñ€ÑƒĞµĞ¼
            result = func(*args, **kwargs)
            cache.set(cache_key, result, ttl=ttl)
            
            return result
        
        return wrapper
    return decorator

def _generate_cache_key(prefix: str, func_name: str, args, kwargs) -> str:
    """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ ĞºÑÑˆĞ°."""
    # Ğ¡ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    args_str = json.dumps({'args': args, 'kwargs': kwargs}, sort_keys=True)
    args_hash = hashlib.md5(args_str.encode()).hexdigest()
    
    return f"{prefix}:{func_name}:{args_hash}"
```

**Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:**

```python
# agents/researcher.py

from infrastructure.distributed_cache import cached

class ResearcherAgent:
    
    @cached(ttl=7200, key_prefix="research")  # 2 Ñ‡Ğ°ÑĞ°
    def research(self, query: str, intent_type: str, **kwargs) -> str:
        """Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼."""
        # ... ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° ...
        return context
```

**Checklist:**
- [ ] Redis Ğ´Ğ»Ñ ĞºÑÑˆĞ°
- [ ] Distributed cache ĞºĞ»Ğ°ÑÑ
- [ ] Ğ”ĞµĞºĞ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€ @cached
- [ ] ĞšÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
- [ ] Cache invalidation ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ
- [ ] ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ hit rate

---

### Ğ¤Ğ°Ğ·Ğ° 3: Database Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (1-2 Ğ½ĞµĞ´ĞµĞ»Ğ¸)

**Ğ¦ĞµĞ»ÑŒ:** Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº

**Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ:** PostgreSQL

```python
# infrastructure/database.py
"""PostgreSQL database Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸."""

from sqlalchemy import create_engine, Column, Integer, String, JSON, DateTime, Enum
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import enum
from datetime import datetime

Base = declarative_base()

class TaskStatus(enum.Enum):
    QUEUED = "queued"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class Task(Base):
    """ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."""
    __tablename__ = 'tasks'
    
    id = Column(String, primary_key=True)
    user_id = Column(String, nullable=True)  # Ğ”Ğ»Ñ multi-user
    status = Column(Enum(TaskStatus), default=TaskStatus.QUEUED)
    
    # Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    task_description = Column(String, nullable=False)
    intent_type = Column(String)
    complexity = Column(String)
    model = Column(String)
    
    # Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
    result = Column(JSON, nullable=True)
    
    # ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    created_at = Column(DateTime, default=datetime.utcnow)
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
    
    # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
    duration_seconds = Column(Integer, nullable=True)
    quality_score = Column(Integer, nullable=True)

class User(Base):
    """ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."""
    __tablename__ = 'users'
    
    id = Column(String, primary_key=True)
    email = Column(String, unique=True)
    api_key = Column(String, unique=True)
    
    # ĞšĞ²Ğ¾Ñ‚Ñ‹
    monthly_quota = Column(Integer, default=1000)
    used_quota = Column(Integer, default=0)
    
    created_at = Column(DateTime, default=datetime.utcnow)

# Database session
engine = create_engine('postgresql://user:pass@localhost/cursor_killer')
SessionLocal = sessionmaker(bind=engine)

def get_db():
    """Dependency Ğ´Ğ»Ñ FastAPI."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

**Checklist:**
- [ ] PostgreSQL setup
- [ ] SQLAlchemy models
- [ ] Alembic migrations
- [ ] CRUD operations
- [ ] Connection pooling
- [ ] Backup strategy

---

### Ğ¤Ğ°Ğ·Ğ° 4: Horizontal Scaling + Load Balancer (1 Ğ½ĞµĞ´ĞµĞ»Ñ)

**Ğ¦ĞµĞ»ÑŒ:** ĞĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ¾Ğ² API Ğ·Ğ° load balancer

**nginx config:**

```nginx
# /etc/nginx/sites-available/cursor-killer

upstream api_backend {
    least_conn;  # Least connections Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ°
    
    server 127.0.0.1:8001 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8002 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8003 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name api.cursor-killer.com;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    limit_req zone=api_limit burst=20 nodelay;
    
    # Timeouts
    proxy_connect_timeout 10s;
    proxy_send_timeout 600s;
    proxy_read_timeout 600s;
    
    location /api/ {
        proxy_pass http://api_backend;
        
        # Headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # SSE support
        proxy_buffering off;
        proxy_cache off;
        chunked_transfer_encoding on;
    }
    
    # Static files
    location / {
        root /var/www/cursor-killer/frontend/dist;
        try_files $uri /index.html;
    }
}
```

**Systemd Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ¾Ğ²:**

```ini
# /etc/systemd/system/cursor-killer-api@.service

[Unit]
Description=Cursor Killer API Instance %i
After=network.target

[Service]
Type=notify
User=cursor
WorkingDirectory=/opt/cursor-killer
Environment="PORT=800%i"

ExecStart=/opt/cursor-killer/.venv/bin/uvicorn \
    backend.api:app \
    --host 0.0.0.0 \
    --port $PORT \
    --workers 1 \
    --log-level info

Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

```bash
# Ğ—Ğ°Ğ¿ÑƒÑĞº 3 Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ¾Ğ²
systemctl enable cursor-killer-api@1
systemctl enable cursor-killer-api@2
systemctl enable cursor-killer-api@3

systemctl start cursor-killer-api@{1,2,3}
```

**Checklist:**
- [ ] nginx setup
- [ ] Systemd services
- [ ] Health checks
- [ ] Session affinity (if needed)
- [ ] SSL/TLS (certbot)
- [ ] Monitoring (Prometheus)

---

### Ğ¤Ğ°Ğ·Ğ° 5: Ollama Cluster (2-3 Ğ½ĞµĞ´ĞµĞ»Ğ¸)

**Ğ¦ĞµĞ»ÑŒ:** ĞĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² Ollama Ğ´Ğ»Ñ LLM

**ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Ollama Load Balancer             â”‚
â”‚  (Round-robin Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ğ¼Ğ¸ Ollama)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚            â”‚
    v           v            v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Ollama 1â”‚  â”‚Ollama 2â”‚  â”‚Ollama 3â”‚
â”‚GPU 0-1 â”‚  â”‚GPU 2-3 â”‚  â”‚GPU 4-5 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# infrastructure/ollama_cluster.py
"""ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ Ollama ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹."""

from typing import List, Optional
import httpx
import random
from dataclasses import dataclass

@dataclass
class OllamaNode:
    """Ğ£Ğ·ĞµĞ» ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ollama."""
    host: str
    port: int
    gpu_ids: List[int]
    max_concurrent: int = 2
    current_load: int = 0

class OllamaCluster:
    """ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ollama."""
    
    def __init__(self):
        self.nodes: List[OllamaNode] = [
            OllamaNode(host="gpu1.local", port=11434, gpu_ids=[0, 1]),
            OllamaNode(host="gpu2.local", port=11434, gpu_ids=[2, 3]),
            OllamaNode(host="gpu3.local", port=11434, gpu_ids=[4, 5]),
        ]
        self.client = httpx.AsyncClient()
    
    def get_available_node(self) -> Optional[OllamaNode]:
        """ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ ÑƒĞ·ĞµĞ» (least loaded)."""
        available = [n for n in self.nodes if n.current_load < n.max_concurrent]
        
        if not available:
            return None
        
        # Least loaded
        return min(available, key=lambda n: n.current_load)
    
    async def generate(self, model: str, prompt: str, **kwargs):
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ ÑƒĞ·ĞµĞ»."""
        node = self.get_available_node()
        
        if not node:
            raise Exception("Ğ’ÑĞµ ÑƒĞ·Ğ»Ñ‹ Ğ·Ğ°Ğ½ÑÑ‚Ñ‹")
        
        try:
            node.current_load += 1
            
            response = await self.client.post(
                f"http://{node.host}:{node.port}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    **kwargs
                },
                timeout=600.0
            )
            
            return response.json()
            
        finally:
            node.current_load -= 1
    
    async def health_check(self) -> Dict[str, bool]:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒĞµ ÑƒĞ·Ğ»Ğ¾Ğ²."""
        health = {}
        
        for node in self.nodes:
            try:
                response = await self.client.get(
                    f"http://{node.host}:{node.port}/api/tags",
                    timeout=5.0
                )
                health[f"{node.host}:{node.port}"] = response.status_code == 200
            except:
                health[f"{node.host}:{node.port}"] = False
        
        return health
```

**Checklist:**
- [ ] Ollama Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ñ…
- [ ] Load balancer Ğ´Ğ»Ñ Ollama
- [ ] Health checks
- [ ] Failover logic
- [ ] Model sync Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ·Ğ»Ğ°Ğ¼Ğ¸
- [ ] GPU monitoring

---

### Ğ¤Ğ°Ğ·Ğ° 6: Monitoring & Alerting (1 Ğ½ĞµĞ´ĞµĞ»Ñ)

**Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸:**
- **Prometheus** - Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
- **Grafana** - Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´Ñ‹
- **AlertManager** - Ğ°Ğ»ĞµÑ€Ñ‚Ñ‹

```python
# infrastructure/metrics_exporter.py
"""Prometheus metrics exporter."""

from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
task_counter = Counter('tasks_total', 'Total tasks', ['status', 'intent_type'])
task_duration = Histogram('task_duration_seconds', 'Task duration', ['intent_type'])
active_tasks = Gauge('tasks_active', 'Active tasks')
queue_size = Gauge('queue_size', 'Queue size', ['queue_name'])

def record_task_start(intent_type: str):
    """Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."""
    active_tasks.inc()
    task_counter.labels(status='started', intent_type=intent_type).inc()

def record_task_complete(intent_type: str, duration: float, success: bool):
    """Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."""
    active_tasks.dec()
    
    status = 'success' if success else 'failed'
    task_counter.labels(status=status, intent_type=intent_type).inc()
    
    task_duration.labels(intent_type=intent_type).observe(duration)

# Ğ—Ğ°Ğ¿ÑƒÑĞº exporter
if __name__ == '__main__':
    start_http_server(9090)
    
    while True:
        # Update queue sizes
        from infrastructure.task_queue import app
        inspector = app.control.inspect()
        
        active = inspector.active()
        for queue_name, tasks in active.items():
            queue_size.labels(queue_name=queue_name).set(len(tasks))
        
        time.sleep(15)
```

**Grafana dashboard:**

```json
{
  "dashboard": {
    "title": "Cursor Killer Metrics",
    "panels": [
      {
        "title": "Tasks per Minute",
        "targets": [
          {
            "expr": "rate(tasks_total[1m])"
          }
        ]
      },
      {
        "title": "Task Duration (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, task_duration_seconds)"
          }
        ]
      },
      {
        "title": "Active Tasks",
        "targets": [
          {
            "expr": "tasks_active"
          }
        ]
      }
    ]
  }
}
```

**Checklist:**
- [ ] Prometheus setup
- [ ] Metrics exporter
- [ ] Grafana dashboards
- [ ] Alerts (Slack/Email)
- [ ] SLA monitoring

---

## ğŸ¯ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²ÑĞµÑ… Ñ„Ğ°Ğ·

### ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
- âœ… 100+ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹
- âœ… Horizontal scaling (Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ²)
- âœ… < 5s latency Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡
- âœ… Task queues Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸
- âœ… Distributed caching

### ĞĞ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ
- âœ… 99.9% uptime Ñ‡ĞµÑ€ĞµĞ· load balancing
- âœ… Graceful degradation Ğ¿Ñ€Ğ¸ ÑĞ±Ğ¾ÑÑ…
- âœ… Auto-recovery
- âœ… Backup & disaster recovery

### ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³
- âœ… Real-time Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
- âœ… Dashboards Ğ² Grafana
- âœ… Alerts Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ…
- âœ… Logging aggregation

---

## ğŸ’° ĞÑ†ĞµĞ½ĞºĞ° ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹

### ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (100 Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹)

**Ğ¡ĞµÑ€Ğ²ĞµÑ€Ğ°:**
- 3x API (4 CPU, 8GB RAM) = $60/Ğ¼ĞµÑ
- 2x Ollama GPU (RTX 3090) = $200/Ğ¼ĞµÑ (Ğ°Ñ€ĞµĞ½Ğ´Ğ° GPU VPS)
- 1x PostgreSQL (2 CPU, 4GB) = $20/Ğ¼ĞµÑ
- 1x Redis (1 CPU, 2GB) = $10/Ğ¼ĞµÑ

**Ğ˜Ñ‚Ğ¾Ğ³Ğ¾:** ~$290/Ğ¼ĞµÑ

### Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (1000 Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹)

- 5x API = $100/Ğ¼ĞµÑ
- 5x Ollama GPU = $500/Ğ¼ĞµÑ
- 1x PostgreSQL (4 CPU, 8GB) = $40/Ğ¼ĞµÑ
- 1x Redis Cluster = $30/Ğ¼ĞµÑ
- 1x nginx LB = $10/Ğ¼ĞµÑ

**Ğ˜Ñ‚Ğ¾Ğ³Ğ¾:** ~$680/Ğ¼ĞµÑ

---

## ğŸ“‹ Checklist Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸

### ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ°
- [ ] Backup Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
- [ ] Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸
- [ ] Rollback plan

### Ğ¤Ğ°Ğ·Ğ° 1: ĞÑ‡ĞµÑ€ĞµĞ´Ğ¸
- [ ] Redis setup
- [ ] Celery/RQ integration
- [ ] Task endpoints
- [ ] Frontend polling
- [ ] ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹

### Ğ¤Ğ°Ğ·Ğ° 2: ĞšÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
- [ ] Redis cache
- [ ] Distributed cache ĞºĞ»Ğ°ÑÑ
- [ ] Cache decorators
- [ ] Hit rate monitoring

### Ğ¤Ğ°Ğ·Ğ° 3: Database
- [ ] PostgreSQL setup
- [ ] SQLAlchemy models
- [ ] Migrations (Alembic)
- [ ] Backup ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ

### Ğ¤Ğ°Ğ·Ğ° 4: Scaling
- [ ] nginx load balancer
- [ ] Multiple API instances
- [ ] Health checks
- [ ] SSL/TLS

### Ğ¤Ğ°Ğ·Ğ° 5: Ollama Cluster
- [ ] Multiple Ollama servers
- [ ] Load balancer
- [ ] Failover
- [ ] Model sync

### Ğ¤Ğ°Ğ·Ğ° 6: Monitoring
- [ ] Prometheus
- [ ] Grafana
- [ ] Alerts
- [ ] Logging

---

**ĞĞ²Ñ‚Ğ¾Ñ€:** AI Assistant  
**Ğ”Ğ°Ñ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ:** 2026-01-21  
**Ğ’ĞµÑ€ÑĞ¸Ñ:** 1.0
